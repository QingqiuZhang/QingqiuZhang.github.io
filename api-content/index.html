{"posts":[{"title":"Tableau Visualization Tutorials - Table of Contents","content":"Bar Plot Rounded Bar Plot ","link":"https://qingqiuzhang.github.io/ZAGhqFyR5/"},{"title":"Tableau Visulization Tutorials - Rounded Bar Plot","content":"Source Data: Superstore Sales Aim: Create a rounded bar plot of sales in each state. Step 1 Drop State &amp; Sales from data pane to Rows &amp; Columns respectively. Step 2 Drop Measure Names &amp; Measure Values from data pane to Rows &amp; Columns respectively. Step 3 Remove unrelavant items in &quot;Measures Values&quot; Card and &quot;SUM(Sales)&quot; in Columns. Double click in &quot;Measures Values&quot; Card and type in &quot;AVG(0)&quot; Step 4 Change &quot;Marks&quot; card from automatic to line and Drop &quot;Measure Names&quot; in Rows over to &quot;Path&quot;. ","link":"https://qingqiuzhang.github.io/7Rz3JT8j/"},{"title":"World Happiness Report","content":"This is a report about world happiness. In the report, we will visualize the distribution of countries' happiness scores from 2015-2019. Besides, we will analyze features like GDP, Health, Trust, and etc. to explore how they contribute to happiness scores. In case you need the source data, visit this webpage: Data Source Exploring Data The column names of the 5 datasets are different and the meaning of each column is stated as below: Country / Country or region: Name of the country. Region: Region the country belongs to. Happiness Rank / Overall rank: Rank of the country based on the Happiness Score. Happiness Score / Score: A metric measured in 2015 by asking the sampled people the question: &quot;How would you rate your happiness on a scale of 0 to 10 where 10 is the happiest.&quot;. Standard Error: The standard error of the happiness score. Economy (GDP per Capita) / Economy..GDP.per.Capita. / GDP per capita: The extent to which GDP contributes to the calculation of the Happiness Score. Family: The extent to which Family contributes to the calculation of the Happiness Score. Health (Life Expectancy) / Health..Life.Expectancy. / Healthy life expectancy: The extent to which Life expectancy contributed to the calculation of the Happiness Score. Freedom / Freedom to make life choices: The extent to which Freedom contributed to the calculation of the Happiness Score. Trust (Government Corruption) / Trust..Government.Corruption. / Perceptions of corruption: The extent to which Perception of Corruption contributes to Happiness Score. Generosity: The extent to which Generosity contributed to the calculation of the Happiness Score. Dystopia Residual / Dystopia.Residual: The extent to which Dystopia Residual contributed to the calculation of the Happiness Score. Lower Confidence Interval: Lower Confidence Interval of the Happiness Score Upper Confidence Interval: Upper Confidence Interval of the Happiness Score Whisker.high: Upper Whisker of the Happiness Score Whisker.low: Lower Whisker of the Happiness Score Social support: The extent to which Social support contributed to the calculation of the Happiness Score. First we need to have a look at the column names. import pandas as pd import numpy as np years = list(range(2015, 2020)) names = locals() for year in years: names[&quot;happiness&quot; + str(year)] = pd.read_csv(f&quot;{year}.csv&quot;) print(names[&quot;happiness&quot; + str(year)].columns) Index(['Country', 'Region', 'Happiness Rank', 'Happiness Score', 'Standard Error', 'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)', 'Freedom', 'Trust (Government Corruption)', 'Generosity', 'Dystopia Residual'], dtype='object') Index(['Country', 'Region', 'Happiness Rank', 'Happiness Score', 'Lower Confidence Interval', 'Upper Confidence Interval', 'Economy (GDP per Capita)', 'Family', 'Health (Life Expectancy)', 'Freedom', 'Trust (Government Corruption)', 'Generosity', 'Dystopia Residual'], dtype='object') Index(['Country', 'Happiness.Rank', 'Happiness.Score', 'Whisker.high', 'Whisker.low', 'Economy..GDP.per.Capita.', 'Family', 'Health..Life.Expectancy.', 'Freedom', 'Generosity', 'Trust..Government.Corruption.', 'Dystopia.Residual'], dtype='object') Index(['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption'], dtype='object') Index(['Overall rank', 'Country or region', 'Score', 'GDP per capita', 'Social support', 'Healthy life expectancy', 'Freedom to make life choices', 'Generosity', 'Perceptions of corruption'], dtype='object') Then we need to unify the column names. happiness2017.columns = [ &quot;Country&quot;, &quot;Happiness Rank&quot;, &quot;Happiness Score&quot;, &quot;Whisker High&quot;, &quot;Whisker Low&quot;, &quot;Economy (GDP per Capita)&quot;, &quot;Family&quot;, &quot;Health (Life Expectancy)&quot;, &quot;Freedom&quot;, &quot;Generosity&quot;, &quot;Trust (Government Corruption)&quot;, &quot;Dystopia Residual&quot;, ] happiness2018.columns = [ &quot;Happiness Rank&quot;, &quot;Country&quot;, &quot;Happiness Score&quot;, &quot;Economy (GDP per Capita)&quot;, &quot;Social Support&quot;, &quot;Health (Life Expectancy)&quot;, &quot;Freedom&quot;, &quot;Generosity&quot;, &quot;Trust (Government Corruption)&quot;, ] happiness2019.columns = [ &quot;Happiness Rank&quot;, &quot;Country&quot;, &quot;Happiness Score&quot;, &quot;Economy (GDP per Capita)&quot;, &quot;Social Support&quot;, &quot;Health (Life Expectancy)&quot;, &quot;Freedom&quot;, &quot;Generosity&quot;, &quot;Trust (Government Corruption)&quot;, ] # Add Region column to happiness2017, happiness2018, and happiness2019 names = locals() for year in range(2017, 2020): for i in range(2015, 2017): names[&quot;happiness&quot; + str(year)] = names[&quot;happiness&quot; + str(year)].merge( names[&quot;happiness&quot; + str(i)][[&quot;Country&quot;, &quot;Region&quot;]], on=&quot;Country&quot;, how=&quot;left&quot; ) null_index = names[&quot;happiness&quot; + str(year)][ names[&quot;happiness&quot; + str(year)].isnull().T.any() ].index.to_list() for ind in null_index: if names[&quot;happiness&quot; + str(year)].loc[ind, &quot;Region_x&quot;] is np.nan: names[&quot;happiness&quot; + str(year)].loc[ind, &quot;Region_x&quot;] = names[ &quot;happiness&quot; + str(year) ].loc[ind, &quot;Region_y&quot;] names[&quot;happiness&quot; + str(year)] = ( names[&quot;happiness&quot; + str(year)] .drop([&quot;Region_y&quot;], axis=1) .rename(columns={&quot;Region_x&quot;: &quot;Region&quot;}) ) Because in each year, the country being involved in the ranking were different and the names for some countries changed, the Region column would have some null values. We need to mannually fill these null values. happiness2017[happiness2017.isnull().T.any()] Country Happiness Rank Happiness Score Whisker High Whisker Low Economy (GDP per Capita) Family Health (Life Expectancy) Freedom Generosity Trust (Government Corruption) Dystopia Residual Region 32 Taiwan Province of China 33 6.422 6.494596 6.349404 1.433627 1.384565 0.793984 0.361467 0.258360 0.063829 2.126607 NaN 70 Hong Kong S.A.R., China 71 5.472 5.549594 5.394406 1.551675 1.262791 0.943062 0.490969 0.374466 0.293934 0.554633 NaN happiness2017.loc[32, &quot;Region&quot;] = 'Eastern Asia' happiness2017.loc[70, &quot;Region&quot;] = 'Eastern Asia' happiness2018[happiness2018.isnull().T.any()] Happiness Rank Country Happiness Score Economy (GDP per Capita) Social Support Health (Life Expectancy) Freedom Generosity Trust (Government Corruption) Region 19 20 United Arab Emirates 6.774 2.096 0.776 0.670 0.284 0.186 NaN Middle East and Northern Africa 37 38 Trinidad &amp; Tobago 6.192 1.223 1.492 0.564 0.575 0.171 0.019 NaN 57 58 Northern Cyprus 5.835 1.229 1.211 0.909 0.495 0.179 0.154 NaN happiness2018.loc[37, &quot;Region&quot;] = 'Latin America and Caribbean' happiness2018.loc[57, &quot;Region&quot;] = 'Western Europe' happiness2019[happiness2019.isnull().T.any()] Happiness Rank Country Happiness Score Economy (GDP per Capita) Social Support Health (Life Expectancy) Freedom Generosity Trust (Government Corruption) Region 38 39 Trinidad &amp; Tobago 6.192 1.231 1.477 0.713 0.489 0.185 0.016 NaN 63 64 Northern Cyprus 5.718 1.263 1.252 1.042 0.417 0.191 0.162 NaN 83 84 North Macedonia 5.274 0.983 1.294 0.838 0.345 0.185 0.034 NaN 119 120 Gambia 4.516 0.308 0.939 0.428 0.382 0.269 0.167 NaN happiness2019.loc[38, &quot;Region&quot;] = 'Latin America and Caribbean' happiness2019.loc[63, &quot;Region&quot;] = 'Western Europe' happiness2019.loc[83, &quot;Region&quot;] = 'Central and Eastern Europe' happiness2019.loc[119, &quot;Region&quot;] = 'Sub-Saharan Africa' Now let's look at the indicators of each dataset. print(happiness2015.shape) happiness2015.describe() (158, 12) Happiness Rank Happiness Score Standard Error Economy (GDP per Capita) Family Health (Life Expectancy) Freedom Trust (Government Corruption) Generosity Dystopia Residual count 158.000000 158.000000 158.000000 158.000000 158.000000 158.000000 158.000000 158.000000 158.000000 158.000000 mean 79.493671 5.375734 0.047885 0.846137 0.991046 0.630259 0.428615 0.143422 0.237296 2.098977 std 45.754363 1.145010 0.017146 0.403121 0.272369 0.247078 0.150693 0.120034 0.126685 0.553550 min 1.000000 2.839000 0.018480 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.328580 25% 40.250000 4.526000 0.037268 0.545808 0.856823 0.439185 0.328330 0.061675 0.150553 1.759410 50% 79.500000 5.232500 0.043940 0.910245 1.029510 0.696705 0.435515 0.107220 0.216130 2.095415 75% 118.750000 6.243750 0.052300 1.158448 1.214405 0.811013 0.549092 0.180255 0.309883 2.462415 max 158.000000 7.587000 0.136930 1.690420 1.402230 1.025250 0.669730 0.551910 0.795880 3.602140 print(happiness2016.shape) happiness2016.describe() (157, 13) Happiness Rank Happiness Score Lower Confidence Interval Upper Confidence Interval Economy (GDP per Capita) Family Health (Life Expectancy) Freedom Trust (Government Corruption) Generosity Dystopia Residual count 157.000000 157.000000 157.000000 157.000000 157.000000 157.000000 157.000000 157.000000 157.000000 157.000000 157.000000 mean 78.980892 5.382185 5.282395 5.481975 0.953880 0.793621 0.557619 0.370994 0.137624 0.242635 2.325807 std 45.466030 1.141674 1.148043 1.136493 0.412595 0.266706 0.229349 0.145507 0.111038 0.133756 0.542220 min 1.000000 2.905000 2.732000 3.078000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.817890 25% 40.000000 4.404000 4.327000 4.465000 0.670240 0.641840 0.382910 0.257480 0.061260 0.154570 2.031710 50% 79.000000 5.314000 5.237000 5.419000 1.027800 0.841420 0.596590 0.397470 0.105470 0.222450 2.290740 75% 118.000000 6.269000 6.154000 6.434000 1.279640 1.021520 0.729930 0.484530 0.175540 0.311850 2.664650 max 157.000000 7.526000 7.460000 7.669000 1.824270 1.183260 0.952770 0.608480 0.505210 0.819710 3.837720 print(happiness2017.shape) happiness2017.describe() (155, 13) Happiness Rank Happiness Score Whisker High Whisker Low Economy (GDP per Capita) Family Health (Life Expectancy) Freedom Generosity Trust (Government Corruption) Dystopia Residual count 155.000000 155.000000 155.000000 155.000000 155.000000 155.000000 155.000000 155.000000 155.000000 155.000000 155.000000 mean 78.000000 5.354019 5.452326 5.255713 0.984718 1.188898 0.551341 0.408786 0.246883 0.123120 1.850238 std 44.888751 1.131230 1.118542 1.145030 0.420793 0.287263 0.237073 0.149997 0.134780 0.101661 0.500028 min 1.000000 2.693000 2.864884 2.521116 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.377914 25% 39.500000 4.505500 4.608172 4.374955 0.663371 1.042635 0.369866 0.303677 0.154106 0.057271 1.591291 50% 78.000000 5.279000 5.370032 5.193152 1.064578 1.253918 0.606042 0.437454 0.231538 0.089848 1.832910 75% 116.500000 6.101500 6.194600 6.006527 1.318027 1.414316 0.723008 0.516561 0.323762 0.153296 2.144654 max 155.000000 7.537000 7.622030 7.479556 1.870766 1.610574 0.949492 0.658249 0.838075 0.464308 3.117485 print(happiness2018.shape) happiness2018.describe() (156, 10) Happiness Rank Happiness Score Economy (GDP per Capita) Social Support Health (Life Expectancy) Freedom Generosity Trust (Government Corruption) count 156.000000 156.000000 156.000000 156.000000 156.000000 156.000000 156.000000 155.000000 mean 78.500000 5.375917 0.891449 1.213237 0.597346 0.454506 0.181006 0.112000 std 45.177428 1.119506 0.391921 0.302372 0.247579 0.162424 0.098471 0.096492 min 1.000000 2.905000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 39.750000 4.453750 0.616250 1.066750 0.422250 0.356000 0.109500 0.051000 50% 78.500000 5.378000 0.949500 1.255000 0.644000 0.487000 0.174000 0.082000 75% 117.250000 6.168500 1.197750 1.463000 0.777250 0.578500 0.239000 0.137000 max 156.000000 7.632000 2.096000 1.644000 1.030000 0.724000 0.598000 0.457000 print(happiness2019.shape) happiness2019.describe() (156, 10) Happiness Rank Happiness Score Economy (GDP per Capita) Social Support Health (Life Expectancy) Freedom Generosity Trust (Government Corruption) count 156.000000 156.000000 156.000000 156.000000 156.000000 156.000000 156.000000 156.000000 mean 78.500000 5.407096 0.905147 1.208814 0.725244 0.392571 0.184846 0.110603 std 45.177428 1.113120 0.398389 0.299191 0.242124 0.143289 0.095254 0.094538 min 1.000000 2.853000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 39.750000 4.544500 0.602750 1.055750 0.547750 0.308000 0.108750 0.047000 50% 78.500000 5.379500 0.960000 1.271500 0.789000 0.417000 0.177500 0.085500 75% 117.250000 6.184500 1.232500 1.452500 0.881750 0.507250 0.248250 0.141250 max 156.000000 7.769000 1.684000 1.624000 1.141000 0.631000 0.566000 0.453000 Visualization Names for some countries in Pyecharts differed from those in the datasets, so we must change the name of some countries into those Pyecharts can recognize. countries = { &quot;South Korea&quot;: &quot;Korea&quot;, &quot;Congo (Kinshasa)&quot;: &quot;Dem. Rep. Congo&quot;, &quot;Congo (Brazzaville)&quot;: &quot;Congo&quot;, &quot;South Sudan&quot;: &quot;S. Sudan&quot;, &quot;Somaliland region&quot;: &quot;Somalia&quot;, &quot;Central African Republic&quot;: &quot;Central African Rep.&quot;, &quot;Ivory Coast&quot;: &quot;Côte d'Ivoire&quot;, &quot;Dominican Republic&quot;: &quot;Dominican Rep.&quot;, &quot;Czech Republic&quot;: &quot;Czech Rep.&quot;, &quot;North Korea&quot;: &quot;Dem. Rep. Korea&quot;, } names = locals() for key, val in countries.items(): for i in range(2015, 2020): try: df_name = names[&quot;happiness&quot; + str(i)] row_ind = int(df_name.loc[df_name[&quot;Country&quot;] == key].index.values) names[&quot;happiness&quot; + str(i)].loc[row_ind, &quot;Country&quot;] = countries[key] except (TypeError, KeyError): pass Put all info into a dictionary for visualization. data = dict() data[2015] = happiness2015.to_dict(&quot;list&quot;) data[2016] = happiness2016.to_dict(&quot;list&quot;) data[2017] = happiness2017.to_dict(&quot;list&quot;) data[2018] = happiness2018.to_dict(&quot;list&quot;) data[2019] = happiness2019.to_dict(&quot;list&quot;) from pyecharts.charts import Map, Timeline, Tab from pyecharts import options as opts import math col_list = [ &quot;Happiness Score&quot;, &quot;Economy (GDP per Capita)&quot;, &quot;Family&quot;, &quot;Health (Life Expectancy)&quot;, &quot;Freedom&quot;, &quot;Trust (Government Corruption)&quot;, &quot;Generosity&quot;, &quot;Dystopia Residual&quot;, ] tab = Tab() for col in col_list: tl = Timeline() for year in range(2015, 2020): try: data_pair = [] # range of values for each feature minimum = min(names[&quot;happiness&quot; + str(year)][col].round(2)) maximum = max(names[&quot;happiness&quot; + str(year)][col].round(2)) data_pair = [ list(z) for z in zip( names[&quot;happiness&quot; + str(year)][&quot;Country&quot;].to_list(), names[&quot;happiness&quot; + str(year)][col].round(3), ) ] c = ( Map() .add(col, data_pair, &quot;world&quot;, is_map_symbol_show=False) .set_series_opts(label_opts=opts.LabelOpts(is_show=False)) .set_global_opts( title_opts=opts.TitleOpts(title=f&quot;World {col} Index of {year}&quot;), visualmap_opts=opts.VisualMapOpts( min_=minimum, max_=maximum, is_piecewise=False ), legend_opts=opts.LegendOpts(is_show=False), ) ) tl = tl.add(c, year).add_schema( is_auto_play=False, play_interval=1000, is_loop_play=True ) except KeyError: pass tab = tab.add(tl, col) tab.render_notebook() # Click tab above the charts to see the distribution of features. # And click the timeline below the charts to swich between years. # Click the play button to automatically see the changes between years. In developed countries, like those in North America, Oceania, and Europe, happiness scores are higher than countries in Asia and Africa. Situations are similar for GDP, Freedom, Trust, and Generosity. So we guess, higher GDP, Freedom, Trust, and Generosity correlated with higher happiness score. World Family Index didn't differ much across countries. But in war region, like Iraq and Afghanistan, and in poor countries, like Sudan and some other countries in Africa, the values are low. Health Index shows similar distribution. Dystopia Residual Index is extremely high in Latin America and in the other countries, there weren't much difference. from pyecharts.charts import Scatter from pyecharts.commons.utils import JsCode col_list = [ &quot;Economy (GDP per Capita)&quot;, &quot;Family&quot;, &quot;Health (Life Expectancy)&quot;, &quot;Freedom&quot;, &quot;Trust (Government Corruption)&quot;, &quot;Generosity&quot;, &quot;Dystopia Residual&quot;, ] tab = Tab() for col in col_list: tl = Timeline() # tooltip format js_code_str = &quot;&quot;&quot; function(params){ return params.data[4]+' - '+params.data[3]+'&lt;br/&gt;' +'X: '+params.data[5]+'&lt;br/&gt;' +'Y: '+params.data[1]; } &quot;&quot;&quot; for year in range(2015, 2020): try: df = names[&quot;happiness&quot; + str(year)] region_list = df[&quot;Region&quot;].unique().tolist() df[&quot;R_transfromed&quot;] = df[&quot;Region&quot;].apply(lambda x: region_list.index(x)) y_data = [ z for z in zip( df[col].round(3), df[&quot;R_transfromed&quot;].to_list(), df[&quot;Country&quot;], df[&quot;Region&quot;], df[&quot;Happiness Score&quot;].round(3), ) ] c = ( Scatter() .add_xaxis(df[&quot;Happiness Score&quot;].to_list()) .add_yaxis(&quot;&quot;, y_data) .set_global_opts( title_opts=opts.TitleOpts( title=f&quot;Relationship between Happiness Score and {col} in {year}&quot; ), visualmap_opts=[ opts.VisualMapOpts( is_show=False, type_=&quot;color&quot;, dimension=2, min_=0, max_=10 ) ], xaxis_opts=opts.AxisOpts( type_=&quot;value&quot;, name=&quot;Happiness Score&quot;, is_scale=True, name_location=&quot;middle&quot;, ), yaxis_opts=opts.AxisOpts(type_=&quot;value&quot;, name=col, is_scale=True), tooltip_opts=opts.TooltipOpts(formatter=JsCode(js_code_str)), ) .set_series_opts(label_opts=opts.LabelOpts(is_show=False)) ) tl = tl.add(c, year).add_schema( is_auto_play=False, play_interval=1000, is_loop_play=True ) except KeyError: pass tab = tab.add(tl, col) tab.render_notebook() From the scatter plot, we further prove our guess that higher GDP and Health Index correlated with higher happiness score. However, higher Freedom Index and Generosity Index dont't mean higher Happiness Score. For Trust Index, when it is smaller than 6, it doesn't show much correlation with happiness score; while when it is greater than 6, the positive correlation is much stronger. Family Index and Dystopia Residual Index also show a positive correlation with Happiness Score to some extent. Countries in North America, Western Europe, and Latin America as well as Australia, New Zealand, Singapore, and some countries in Middle East are ranked as the happiest countries among the world, most of which are also among the richest countries. As for countries in Latin America, I think citizens' happiness may be rooted in their heart and naturally part of their nature. This might be explained by the enthusiasm and optimism. ","link":"https://qingqiuzhang.github.io/4QfPkvU3t/"},{"title":"Diabetes Data Analytics (2) -- Model Training, Evaluating and Testing","content":"In the previous article, I did data preprocessing. It is often seen as one of the most important step during a machine learning project. One must be patient with the data manimulation no matter how troublesome it is. In this article, we are finally exposed to the model traing procedure. I will demonstrate how to conduct different types of classification models and how to evalute their performance. For everything you may need, visit: Source Data Hope this helps! In Daibetes Data Analytics (1) -- Data Preprocessing, I demonstrate how to do feature scaling and deature reductio separately. This time I'll show how to combine these steps with model training using Pipeline. Model Training And Evaluating There are so many classification models. Some you may have heard or used while the others not. This articles covers almost all the classification models you may need as a beginner. I will show you how to conduct them and how different hyperparameters will affect the models. Notice the X and y used in train_test_split are the same from here. from sklearn.model_selection import GridSearchCV from sklearn.model_selection import ShuffleSplit from sklearn.model_selection import cross_validate # get traing set and testing set X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = 0.25, random_state = 42 ) # Since this is a clinical dataset, we don't care too much about accuracy. # Recall and precision are more important. So we use f1 score to evaluate models # cross validation def Cval(X_train, y_train, modelObj): # split training set into traing set &amp; validation set shuffle = ShuffleSplit(n_splits=100, test_size=0.25, random_state=10) CVInfo = cross_validate( modelObj, X_train, y_train, cv = shuffle, scoring = 'f1', return_train_score = True, n_jobs = -1 ) mean_train = np.mean(CVInfo['train_score']) mean_test = np.mean(CVInfo['test_score']) return f&quot;Mean Train: {mean_train}, Mean Test: {mean_test}&quot; # grid search def Gsearch(X, y, modelObj): # split training set into traing set &amp; validation set shuffle = ShuffleSplit(n_splits=100, test_size=0.25, random_state=10) grid_search = GridSearchCV( modelObj, param_grid, cv = shuffle, scoring = 'f1', return_train_score = True, n_jobs = -1 ) grid_search.fit(X,y) results = pd.DataFrame(grid_search.cv_results_) return results K-Nearest Neighbors Classifier from sklearn.pipeline import Pipeline from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier # grid search: K klist = list(range(1, 20)) fullModel = Pipeline( [ (&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;knn&quot;,KNeighborsClassifier()) ] ) param_grid = {'knn__n_neighbors':klist} print( Gsearch(X_train, y_train, fullModel)[ [ 'rank_test_score', 'mean_train_score', 'mean_test_score', 'param_knn__n_neighbors' ] ] ) rank_test_score mean_train_score mean_test_score param_knn__n_neighbors 0 17 1.000000 0.767296 1 1 19 0.905732 0.705512 2 2 15 0.876778 0.778121 3 3 18 0.846312 0.749821 4 4 14 0.845480 0.782354 5 5 16 0.829343 0.775068 6 6 2 0.834093 0.789197 7 7 12 0.831851 0.785771 8 8 4 0.830803 0.787731 9 9 9 0.829832 0.786638 10 10 1 0.825560 0.790609 11 11 13 0.823216 0.785650 12 12 3 0.820105 0.788285 13 13 11 0.818992 0.786504 14 14 5 0.816281 0.787701 15 15 7 0.815164 0.786984 16 16 6 0.812760 0.787472 17 17 8 0.813046 0.786949 18 18 10 0.810027 0.786559 19 Logistic Regression Without Penalty from sklearn.linear_model import LogisticRegression fullModel = Pipeline( [ (&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;lr&quot;, LogisticRegression(penalty=&quot;none&quot;)), ] ) print(Cval(X_train, y_train, fullModel)) Mean Train: 0.7489133929356746, Mean Test: 0.7334838216167244 With &quot;l2&quot; Penalty clist = [0.1, 0.5, 1., 2., 5., 10., 50., 100., 200.] fullModel = Pipeline( [ (&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;lr&quot;, LogisticRegression(penalty=&quot;l2&quot;)), ] ) param_grid = {&quot;lr__C&quot;: clist} print( Gsearch(X_train, y_train, fullModel)[ [&quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_lr__C&quot;] ] ) rank_test_score mean_train_score mean_test_score param_lr__C 0 9 0.746639 0.732332 0.1 1 1 0.748524 0.734091 0.5 2 2 0.748749 0.733897 1.0 3 4 0.748788 0.733610 2.0 4 3 0.748850 0.733614 5.0 5 8 0.748915 0.733403 10.0 6 7 0.748927 0.733416 50.0 7 5 0.748927 0.733484 100.0 8 5 0.748913 0.733484 200.0 Naive Bayes Classifier from sklearn.naive_bayes import GaussianNB fullModel = Pipeline( [(&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;gnb&quot;, GaussianNB())] ) print(Cval(X_train, y_train, fullModel)) Mean Train: 0.7156724204600774, Mean Test: 0.6935958660647001 Support Vector Machine Linear from sklearn.svm import SVC C = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 5.0, 10.0] fullModel = Pipeline( [(&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;lsv&quot;, SVC(kernel=&quot;linear&quot;))] ) param_grid = {&quot;lsv__C&quot;: C} print( Gsearch(X_train, y_train, fullModel)[ [&quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_lsv__C&quot;] ] ) rank_test_score mean_train_score mean_test_score param_lsv__C 0 9 0.736142 0.721251 0.01 1 8 0.748459 0.732040 0.1 2 7 0.751446 0.734082 1.0 3 3 0.751417 0.734420 1.5 4 4 0.751544 0.734324 2.0 5 1 0.751747 0.734826 5.0 6 2 0.751812 0.734491 10.0 7 6 0.751675 0.734110 20.0 8 5 0.751708 0.734141 50.0 Nonlinear C = [0.1, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0] gamma = [0.0001, 0.001, 0.1, 0.5, 1.0, 1.5, 2.0] fullModel = Pipeline( [(&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;nlsv&quot;, SVC(kernel=&quot;rbf&quot;))] ) param_grid = {&quot;nlsv__C&quot;: C, &quot;nlsv__gamma&quot;: gamma} print( Gsearch(X_train, y_train, fullModel)[ [ &quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_nlsv__C&quot;, &quot;param_nlsv__gamma&quot;, ] ] ) rank_test_score mean_train_score mean_test_score param_nlsv__C param_nlsv__gamma 0 44 0.372066 0.351788 0.1 0.0001 1 43 0.372777 0.352342 0.1 0.001 2 16 0.794937 0.769077 0.1 0.1 3 19 0.796141 0.759841 0.1 0.5 4 38 0.506495 0.426928 0.1 1.0 5 41 0.389886 0.354617 0.1 1.5 6 49 0.389178 0.338997 0.1 2.0 7 44 0.372066 0.351788 0.25 0.0001 8 37 0.512152 0.495792 0.25 0.001 9 9 0.819667 0.787810 0.25 0.1 10 12 0.865541 0.782728 0.25 0.5 11 20 0.898090 0.747326 0.25 1.0 12 36 0.816284 0.555991 0.25 1.5 13 39 0.589468 0.395044 0.25 2.0 14 44 0.372066 0.351788 0.5 0.0001 15 33 0.715235 0.699347 0.5 0.001 16 7 0.832132 0.790794 0.5 0.1 17 3 0.906015 0.792512 0.5 0.5 18 14 0.953103 0.775851 0.5 1.0 19 25 0.978297 0.724190 0.5 1.5 20 35 0.989296 0.616334 0.5 2.0 21 44 0.372066 0.351788 0.75 0.0001 22 31 0.723086 0.707281 0.75 0.01 23 6 0.840006 0.791039 0.75 0.1 24 5 0.920507 0.791277 0.75 0.5 25 13 0.965587 0.777566 0.75 1.0 26 24 0.986410 0.732414 0.75 1.5 27 34 0.995198 0.669428 0.75 2.0 28 44 0.372066 0.351788 1.0 0.0001 29 28 0.728932 0.711763 1.0 0.01 30 4 0.845415 0.791687 1.0 0.1 31 8 0.930660 0.790740 1.0 0.5 32 15 0.975305 0.774491 1.0 1.0 33 21 0.992273 0.741896 1.0 1.5 34 32 0.996811 0.706107 1.0 2.0 35 42 0.372209 0.352824 1.5 0.0001 36 26 0.736768 0.720368 1.5 0.01 37 2 0.852950 0.793025 1.5 0.1 38 10 0.947417 0.787222 1.5 0.5 39 17 0.987013 0.768417 1.5 1.0 40 22 0.996314 0.741427 1.5 1.5 41 29 0.998739 0.710655 1.5 2.0 42 40 0.405951 0.387968 2.0 0.0001 43 27 0.735862 0.718574 2.0 0.01 44 1 0.858846 0.793716 2.0 0.1 45 11 0.958640 0.783840 2.0 0.5 46 18 0.992780 0.765268 2.0 1.0 47 23 0.998327 0.740466 2.0 1.5 48 30 0.999947 0.709506 2.0 2.0 Decision Tree from sklearn.tree import DecisionTreeClassifier max_tree_depth = list(np.arange(1, 11, 1)) fullModel = Pipeline( [(&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;dtc&quot;, DecisionTreeClassifier())] ) param_grid = {&quot;dtc__max_depth&quot;: max_tree_depth} print( Gsearch(X_train, y_train, fullModel)[ [ &quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, ] ] ) rank_test_score mean_train_score mean_test_score param_dtc__max_depth 0 3 0.769956 0.752233 1 1 5 0.766799 0.739621 2 2 10 0.777782 0.728209 3 3 1 0.822327 0.759595 4 4 2 0.851185 0.753298 5 5 4 0.882829 0.746050 6 6 6 0.912364 0.739208 7 7 7 0.937076 0.737403 8 8 9 0.956295 0.729259 9 9 8 0.971882 0.730771 10 Random Forest max_tree_depth = list(np.arange(1, 21, 1)) fullModel = Pipeline( [(&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;forest&quot;, RandomForestClassifier())] ) param_grid = {&quot;forest__max_depth&quot;: max_tree_depth} print( Gsearch(X_train, y_train, fullModel)[ [ &quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, ] ] ) rank_test_score mean_train_score mean_test_score param_forest__max_depth 0 20 0.780006 0.743256 1 1 19 0.810001 0.760425 2 2 18 0.832622 0.770300 3 3 17 0.857490 0.776023 4 4 16 0.885679 0.782947 5 5 14 0.916162 0.789779 6 6 15 0.946883 0.789271 7 7 13 0.970423 0.793335 8 8 7 0.985119 0.795747 9 9 6 0.992331 0.795925 10 10 12 0.996899 0.793636 11 11 9 0.999103 0.795075 12 12 8 0.999682 0.795129 13 13 1 0.999947 0.798139 14 14 2 0.999982 0.796966 15 15 5 0.999982 0.796176 16 16 10 1.000000 0.794725 17 17 11 1.000000 0.794472 18 18 4 1.000000 0.796287 19 19 3 1.000000 0.796903 20 Model Testing For now we have trained a set of models, among which KNN, non-linear SVC, and Random Forest are the best. So we use the three methods to see how they perform on testing set. K-Nearest Neighbors Classifier from sklearn.metrics import f1_score fullModel = Pipeline( [ (&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;nlsv&quot;, KNeighborsClassifier(n_neighbors=11)), ] ) fullModel.fit(X_train, y_train) y_pred = fullModel.predict(X_test) f1_score(y_test, y_pred) 0.793103448275862 Non-linear Support Vector Classifier fullModel = Pipeline( [ (&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;nlsv&quot;, SVC(kernel=&quot;rbf&quot;, C=1, gamma=0.5)), ] ) fullModel.fit(X_train, y_train) y_pred = fullModel.predict(X_test) f1_score(y_test, y_pred) 0.8060836501901141 Random Forest fullModel = Pipeline( [ (&quot;scaler&quot;, StandardScaler()), (&quot;pca&quot;, PCA()), (&quot;forest&quot;, RandomForestClassifier(max_depth=14)), ] ) fullModel.fit(X_train, y_train) y_pred = fullModel.predict(X_test) f1_score(y_test, y_pred) 0.8057553956834532","link":"https://qingqiuzhang.github.io/diabetes-dataset-report-2-model-training-evaluating-and-testing/"},{"title":"Diabetes Data Analytics (1) -- Data Preprocessing","content":"This is a thorough analysis of diabetes dataset from Kaggle. In this article, I will demonstrate how to do an end-to-end machine learning project using diverse classification models including K-Nearest Neighbors, Logistic Regression, Support Vector Machine, Decision Tree and Random Forest. For everything you may need, visit: Source Data Hope this helps! I cannot emphasize the importance of data preprocessing. It is the process of transforming raw data into more reasonable, useful and efficient format. It's a must and one of the most important step in a machine learing project. Without it our models will probably crash and won't be able to generate good results. There are several steps in data preprocessing: data cleaning, data reduction and data transformation. In the following paragraphs, I'll show you how to perform the steps one by one. Data Cleaning First, I load the data from the .csv file and print the first ten rows so that I can have a basic understanding of how the data looks like. # packages needed import numpy as np import pandas as pd # load data data = pd.read_csv('diabetes.csv') data.head(10) Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 5 5 116 74 0 0 25.6 0.201 30 0 6 3 78 50 32 88 31.0 0.248 26 1 7 10 115 0 0 0 35.3 0.134 29 0 8 2 197 70 45 543 30.5 0.158 53 1 9 8 125 96 0 0 0.0 0.232 54 1 The meaning of each column is stated as below: Pregnancies: Number of times pregnant Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test BloodPressure: Diastolic blood pressure (mm Hg) SkinThickness: Triceps skin fold thickness (mm) Insulin: 2-Hour serum insulin (mu U/ml) BMI: Body mass index (weight in kg/(height in m)^2) DiabetesPedigreeFunction: Diabetes pedigree function Age: Age (years) Outcome: Class variable (0 or 1) Next, we need to examin whether there are null values in the dataset. data.isnull().sum() Pregnancies 0 Glucose 0 BloodPressure 0 SkinThickness 0 Insulin 0 BMI 0 DiabetesPedigreeFunction 0 Age 0 Outcome 0 dtype: int64 It seems there is no null values. Shall we pop the champagne now? If we further examine the data, we will find that the situation is not as good as we thought. data.describe().round(2).T count mean std min 25% 50% 75% max Pregnancies 768.0 3.85 3.37 0.00 1.00 3.00 6.00 17.00 Glucose 768.0 120.89 31.97 0.00 99.00 117.00 140.25 199.00 BloodPressure 768.0 69.11 19.36 0.00 62.00 72.00 80.00 122.00 SkinThickness 768.0 20.54 15.95 0.00 0.00 23.00 32.00 99.00 Insulin 768.0 79.80 115.24 0.00 0.00 30.50 127.25 846.00 BMI 768.0 31.99 7.88 0.00 27.30 32.00 36.60 67.10 DiabetesPedigreeFunction 768.0 0.47 0.33 0.08 0.24 0.37 0.63 2.42 Age 768.0 33.24 11.76 21.00 24.00 29.00 41.00 81.00 Outcome 768.0 0.35 0.48 0.00 0.00 0.00 1.00 1.00 We can see from the picture above that the minimum value of Glucose, BloodPressure, SkinThickness, Insulin, BMI is zero. However, anyone having common sense knows that by no means would these indicators of an alive person be zero. Apparently, the null values are replaced by zeros when inputing data. Here we calculate the number of null values in each column so that we will better decide how to deal with them. for col in list(data.columns): zero = len(data[data[col] == 0]) print('0s in %s: %d' % (col, zero)) 0s in Pregnancies: 111 0s in Glucose: 5 0s in BloodPressure: 35 0s in SkinThickness: 227 0s in Insulin: 374 0s in BMI: 11 0s in DiabetesPedigreeFunction: 0 0s in Age: 0 0s in Outcome: 500 There are nearly 400 null values in Insulin column, so it's not realistic to drop all the null values. Normally under such circumstance, we replace them with indicators such as mean, median, or mode, etc. Here I choose median. # replace 0s in target columns with null value data.iloc[:, 1:6] = data.iloc[:, 1:6].replace({0:np.NaN}) # fill null values with medians data = data.fillna(data.median()) data.describe().round(2).T count mean std min 25% 50% 75% max Pregnancies 768.0 3.85 3.37 0.00 1.00 3.00 6.00 17.00 Glucose 768.0 121.66 30.44 44.00 99.75 117.00 140.25 199.00 BloodPressure 768.0 72.39 12.10 24.00 64.00 72.00 80.00 122.00 SkinThickness 768.0 29.11 8.79 7.00 25.00 29.00 32.00 99.00 Insulin 768.0 140.67 86.38 14.00 121.50 125.00 127.25 846.00 BMI 768.0 32.46 6.88 18.20 27.50 32.30 36.60 67.10 DiabetesPedigreeFunction 768.0 0.47 0.33 0.08 0.24 0.37 0.63 2.42 Age 768.0 33.24 11.76 21.00 24.00 29.00 41.00 81.00 Outcome 768.0 0.35 0.48 0.00 0.00 0.00 1.00 1.00 Pairplot &amp; Correlation Matrix -- Imbalanced Dataset Now we can visualize the dataset and see if we can find something else. import seaborn as sns import matplotlib.pyplot as plt sns.pairplot(data, hue=&quot;Outcome&quot;, corner=True) corr_mat = data.corr().round(2) plt.figure(figsize = (15,10)) sns.heatmap(corr_mat, annot=True) From the pairplot above, we discover this is an imbalanced dataset. There are 500 observations whose outcome is 0 and only 268 is 1. If we train our model with such data, the results may be misleading. There are several methods to deal with imbalanced data: Collect more data Under-Sampling Over-Sampling Use confusion matrix or other ways to evaluate the peformance As the number of observations is only 768, it's unrealistic to under-sampling. Here I wil illustrate how to do oversampling using SMOTE(). Oversampling with SMOTE() We can use SMOTE() to over-sampling observations so that the number of outcome in each class will be the same. After doing that, we describe the data again. from imblearn.over_sampling import SMOTE # get X, y X = data.iloc[:, 0:8] y = data.iloc[:, -1] # perform SMOTE() oversample = SMOTE() X, y = oversample.fit_resample(X, y) # join X, y y = pd.DataFrame(y) data = pd.concat([X, y], axis=1) print(data.shape) data.describe().round(2).T count mean std min 25% 50% 75% max Pregnancies 1000.0 3.99 3.33 0.00 1.00 3.00 6.00 17.00 Glucose 1000.0 126.40 31.39 44.00 102.82 122.00 148.00 199.00 BloodPressure 1000.0 72.97 11.71 24.00 65.75 72.00 80.00 122.00 SkinThickness 1000.0 29.64 8.22 7.00 27.00 29.00 33.00 99.00 Insulin 1000.0 146.58 90.69 14.00 125.00 125.00 136.25 846.00 BMI 1000.0 33.01 6.68 18.20 28.40 32.66 36.80 67.10 DiabetesPedigreeFunction 1000.0 0.49 0.32 0.08 0.26 0.40 0.65 2.42 Age 1000.0 33.88 11.34 21.00 25.00 31.00 41.00 81.00 Outcome 1000.0 0.50 0.50 0.00 0.00 0.50 1.00 1.00 Pairplot &amp; Correlation Matrix -- Balanced Dataset Once again visualize the dataset to see the changes in the distribution of data. sns.pairplot(data, hue=&quot;Outcome&quot;, corner=True) For now, we transform the original imbalanced dataset into a balanced dataset with the number of each outcome class being 500. there is not much difference in distribution between diabetics and nondiabetics. Glucose should have the highest correlation with Outcome among all the features. We can testify it using a correlation matrix heatmap. corr_mat = data.corr().round(2) plt.figure(figsize = (15,10)) sns.heatmap(corr_mat, annot=True) The features with higher correlation with Outcome are more important and more inspirable to our model process. Data Transformation If the range of two features differ a lot, for example, one is from 0 to 1 and the other is from 0 to 10,000, apparantly models won't treat such two features equally. Normally, the feature with larger range would influence the result more. Nevertheless, this doesn't mean it's more important than the other one. That's why we need scale the features. Frequently used scaling techniques are: MinMaxScaler() StardardScaler() MaxAbsScaler() RobustScaler() I usually choose MinMaxScaler() or StandardScaler() for most of datasets. You can also use others according to the charasteristic of the dataset at hand. from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # get X, y X = data.drop([&quot;Outcome&quot;], axis=1) y = data[&quot;Outcome&quot;] X2 = X.copy y2 = y.copy # split X, y into traning set and validation set X2_train, X2_trainValid, y2_train, y2_trainValid = train_test_split(X2, y2, test_size=0.5) # perform feature scaling scaler = StandardScaler() X2_train = scaler.fit_transform(X2_train) print(&quot;The training set:&quot;) print(X2_train) X2_trainValid = scaler.transform(X2_trainValid) print(&quot;The validation set:&quot;) print(X2_trainValid) The training set: [[ 0.91861577 1.16692299 1.18785055 ... -0.37252998 -1.00546758 1.12702163] [-0.31857719 2.14006051 -0.37791467 ... -0.29655845 -0.59256699 -0.00540078] [-1.2464719 -0.54138673 1.3123182 ... -0.07795059 0.92337497 0.25592747] ... [ 0.60931753 0.71279215 0.66592881 ... -0.50927873 -0.96541006 1.38834988] [ 0.91861577 0.69093641 0.49301433 ... 0.36541497 0.17583587 1.4754593 ] [-1.2464719 -1.00641748 -0.20394076 ... 1.61792412 -0.44466231 -1.1378232 ]] The validation set: [[-0.93717366 -1.13616915 -0.89983642 ... -0.85874777 0.2732917 -0.87649495] [-0.62787543 -1.03885539 -0.7258625 ... 0.78223728 0.56293838 -0.96360437] [ 0.30001929 -1.20104498 -0.55188859 ... -1.28418834 -0.46006904 -0.35383845] ... [-0.62787543 -0.94154164 -0.37791467 ... -1.78560044 0.47974199 -0.70227612] [ 0.30001929 0.61547839 0.83990272 ... -0.12942108 -0.1211208 2.08522521] [ 0.60931753 -0.58472455 1.36182446 ... 0.59990561 0.72624981 -0.26672903]] Data Reduction With clean data at hand, we can visulize the data, which may indicate some relations between the features or between features and targets. Knowing these is useful to data reduction, for example, if a feature is highly correlated with the target, then we had better retain it. Or if two features are highly correlated, it would be much easier for us if we drop out one of them. Here I use PCA (Principal componenet analysis) for a demonstration. from sklearn.decomposition import PCA pca = PCA() X2_train = pca.fit_transform(X2_train) print(&quot;The training set:&quot;) print(X2_train) X2_trainValid = pca.transform(X2_trainValid) print(&quot;The validation set:&quot;) print(X2_trainValid) The training set: [[ 1.11094069 1.71567637 0.06106008 ... -0.41295852 0.00731795 -0.44959938] [-0.35117318 0.31229466 1.6917659 ... -1.61109567 -0.49354468 0.3728086 ] [ 0.16735037 -0.4001708 -0.38262459 ... 0.82802415 0.95458562 -0.10766268] ... [ 0.66464415 1.65681528 0.09661504 ... -0.34339558 0.49824185 -0.21578121] [ 1.89124805 0.9469956 0.91665363 ... 0.40528492 0.16728102 0.47349385] [-0.77480095 -1.71331367 -1.18697513 ... 0.26478707 -0.22960668 1.25339887]] The validation set: [[-2.92512666 -0.09229361 0.31554059 ... -0.10102492 -0.22291171 0.68989167] [-0.55971697 -1.49656753 -0.53947916 ... 0.41658342 -0.12598596 0.48539238] [-2.27295931 1.03707233 -0.42829519 ... -0.03041183 -0.23491582 -0.2007138 ] ... [-2.79570134 0.52778715 0.64923584 ... 0.15441194 -0.14783737 -0.13731851] [ 1.54801396 1.38771828 1.28965127 ... 0.67922686 0.85859194 0.66502772] [ 0.65313216 0.2231839 -0.87818227 ... 0.76275166 -0.74002896 0.09427758]] ","link":"https://qingqiuzhang.github.io/diabetes-dataset-report-2-data-preprocessing/"},{"title":"Iris Dataset Analytics","content":"The Iris Data set was created by R.A. Fisher and is perhaps the best known data set to be found in the pattern recognition literature. Fisher’s paper is a classic in the field and is referenced frequently to this day. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. Predicted attribute: class of iris plant. This article aims to classify this dataset utilizing the following models: K Nearest Neighbors (KNN), Naive Bayes and Logistic Regression. You can find the data source and Iris Dataset Analytics.py here. Exploring Data import pandas as pd import numpy as np df = pd.read_csv( &quot;/Users/qingqiuzhang/Desktop/iris_dataset.csv&quot;, header = None, names = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;, &quot;class&quot;], ) df.head() sepal length sepal width petal length petal width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa Calculate some indicators of features. df.describe() sepal length sepal width petal length petal width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 ## Visualization import matplotlib.pyplot as plt import seaborn as sns features = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;] sns.set(style=&quot;ticks&quot;, palette=&quot;pastel&quot;) f, axes = plt.subplots(2, 2, sharey=False, figsize=(14, 14)) for ind, val in enumerate(features): sns.violinplot(x=&quot;class&quot;, y=val, data=df, ax=axes[ind // 2, ind % 2]).set( title = &quot;Sepal Length&quot; ) plt.show() sns.pairplot(df, hue=&quot;class&quot;) Train and evaluate three models using cross-validation K-Nearest Neighbors Classifier from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import ShuffleSplit from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV NUM = 200 X = df.drop([&quot;class&quot;], axis=1) y = df[&quot;class&quot;] shuffle = ShuffleSplit(n_splits=NUM, test_size=0.25, random_state=10) results = [] klist = np.arange(1, 21, 1) FullModel = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;knn&quot;, KNeighborsClassifier())]) param_grid = {&quot;knn__n_neighbors&quot;: klist} grid_search = GridSearchCV( FullModel, param_grid, scoring = &quot;accuracy&quot;, cv = shuffle, return_train_score = True, n_jobs = -1, ) grid_search.fit(X, y) results = pd.DataFrame(grid_search.cv_results_) print( results[ [ &quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_knn__n_neighbors&quot;, ] ] ) rank_test_score mean_train_score mean_test_score param_knn__n_neighbors 0 18 1.000000 0.943947 1 1 20 0.970268 0.937368 2 2 17 0.959018 0.945132 3 3 15 0.958214 0.945789 4 4 12 0.963304 0.949737 5 5 9 0.963080 0.952105 6 6 6 0.966741 0.954605 7 7 3 0.962500 0.955395 8 8 7 0.963482 0.954474 9 9 5 0.963571 0.955000 10 10 2 0.965536 0.956447 11 11 1 0.965982 0.958553 12 12 4 0.963973 0.955132 13 13 8 0.963661 0.954079 14 14 11 0.961205 0.950000 15 15 10 0.959107 0.950658 16 16 14 0.957143 0.946711 17 17 13 0.956205 0.947895 18 18 16 0.955223 0.945526 19 19 19 0.953036 0.942895 20 # Plot Accuracy vs K fig, ax = plt.subplots() ax.plot( results[&quot;param_knn__n_neighbors&quot;], results[&quot;mean_test_score&quot;], label=&quot;test accuracy&quot; ) ax.set_xlim(15, 0) # reverse x; from simple model to complex model # (complex model tries hard to sort of figure out all sorts of details in the data) ax.set_ylabel(&quot;Accuracy&quot;) ax.set_xlabel(&quot;n_neighbors&quot;) ax.grid() ax.legend() Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB from sklearn.preprocessing import MinMaxScaler features = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;] df1 = df.copy() for column in features: df1[column] = [round(i - np.min(df1[column])) for i in df1[column]] df1[column] = df1[column].astype(&quot;category&quot;) X = pd.get_dummies(df1[features]) # Calculate mean test accuracy alphas = [0.01, 0.1, 1.0, 5.0, 10.0, 15.0, 20.0, 35.0, 50.0] FullModel = Pipeline([(&quot;scaler&quot;, MinMaxScaler()), (&quot;mnb&quot;, MultinomialNB())]) param_grid = {&quot;mnb__alpha&quot;: alphas} grid_search = GridSearchCV( FullModel, param_grid, scoring = &quot;accuracy&quot;, cv = shuffle, return_train_score = True, n_jobs = -1, ) grid_search.fit(X, y) results = pd.DataFrame(grid_search.cv_results_) print( results[ [&quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_mnb__alpha&quot;] ] ) rank_test_score mean_train_score mean_test_score param_mnb__alpha 0 9 0.942054 0.931711 0.01 1 8 0.942187 0.932105 0.1 2 7 0.944420 0.935789 1.0 3 5 0.947321 0.942237 5.0 4 3 0.946830 0.945132 10.0 5 1 0.946786 0.946184 15.0 6 2 0.946741 0.946053 20.0 7 4 0.945312 0.944211 35.0 8 6 0.943080 0.941053 50.0 Logistic Regression from sklearn.linear_model import LogisticRegression df2 = df.copy() X = df2.iloc[:, 0:4].values y = df2.iloc[:, 4].values # Calculate mean test accuracy Clist = [0.01, 0.1, 1.0, 5.0, 10.0, 20.0, 50.0, 100.0, 150.0, 200.0] FullModel = Pipeline( [ (&quot;scaler&quot;, StandardScaler()), (&quot;lr&quot;, LogisticRegression(penalty=&quot;l2&quot;, solver=&quot;lbfgs&quot;, multi_class=&quot;auto&quot;)), ] ) param_grid = {&quot;lr__C&quot;: Clist} grid_search = GridSearchCV( FullModel, param_grid, scoring = &quot;accuracy&quot;, cv = shuffle, return_train_score = True, n_jobs = -1, ) grid_search.fit(X, y) results = pd.DataFrame(grid_search.cv_results_) print( results[[&quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_lr__C&quot;]] ) rank_test_score mean_train_score mean_test_score param_lr__C 0 10 0.852321 0.834342 0.01 1 9 0.919777 0.908684 0.1 2 8 0.969509 0.960132 1.0 3 7 0.979420 0.968026 5.0 4 4 0.981295 0.969342 10.0 5 3 0.982857 0.969737 20.0 6 1 0.984598 0.970263 50.0 7 2 0.986205 0.969737 100.0 8 5 0.986786 0.969211 150.0 9 6 0.987009 0.968684 200.0 Conclusion For the current running, Logistic Regression has the highest test accuracy followed by KNN Classifier. Naive Bayes Classifier has the lowest test accuracy. ","link":"https://qingqiuzhang.github.io/iris-data-set-analytics/"},{"title":"Bike Sharing Dataset Analytics (2) -- Hourly Data","content":"In the previous article, I analyze the bike sharing dataset from a daily level. In this article, I will do a similar procedure from a houly level. Source data: hour.csv Source python file: Bike Sharing Dataset Analytics -- Hourly Data.py Exploring the Data import pandas as pd hour = pd.read_csv(&quot;hour.csv&quot;) hour.head() instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 Visualization import matplotlib.pyplot as plt result = hour[[&quot;hr&quot;, &quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].groupby([&quot;hr&quot;]).mean() result = ( result.stack() .reset_index() .set_index(&quot;hr&quot;) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people per hour&quot;}) ) f, axes = plt.subplots(2, sharey=False, figsize=(25, 12)) sns.barplot(x=result.index, y=&quot;people per hour&quot;, hue=&quot;cat&quot;, data=result, ax=axes[0]) result = hour[[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].set_index(hour[&quot;hr&quot;]) result = ( result.stack() .reset_index() .set_index(&quot;hr&quot;) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people&quot;}) ) sns.violinplot(x=result.index, y=&quot;people&quot;, hue=&quot;cat&quot;, data=result, cut=0, ax=axes[1]) Linear Regression Split Data from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler cat = [&quot;season&quot;, &quot;mnth&quot;, &quot;hr&quot;, &quot;holiday&quot;, &quot;weekday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;] hour[cat] = hour[cat].apply(lambda x: x.astype(&quot;category&quot;)) dummies = pd.get_dummies(hour[cat], drop_first=True) print(dummies.shape) conti_predictors = [&quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;] print(hour[conti_predictors].shape) X = pd.concat([hour[conti_predictors], dummies], axis=1) X.head() y = hour.iloc[:, 14:] y.head() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) sc = StandardScaler() X_train[conti_predictors] = sc.fit_transform(X_train[conti_predictors]) X_test[conti_predictors] = sc.transform(X_test[conti_predictors]) (17379, 48) (17379, 4) temp atemp hum windspeed season_2 season_3 season_4 mnth_2 mnth_3 mnth_4 mnth_5 mnth_6 mnth_7 mnth_8 mnth_9 mnth_10 mnth_11 mnth_12 hr_1 hr_2 hr_3 hr_4 hr_5 hr_6 hr_7 hr_8 hr_9 hr_10 hr_11 hr_12 hr_13 hr_14 hr_15 hr_16 hr_17 hr_18 hr_19 hr_20 hr_21 hr_22 hr_23 holiday_1 weekday_1 weekday_2 weekday_3 weekday_4 weekday_5 weekday_6 workingday_1 weathersit_2 weathersit_3 weathersit_4 0 0.24 0.2879 0.81 0.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0.22 0.2727 0.80 0.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0.22 0.2727 0.80 0.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 3 0.24 0.2879 0.75 0.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 4 0.24 0.2879 0.75 0.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 casual registered cnt 0 3 13 16 1 8 32 40 2 5 27 32 3 3 10 13 4 0 1 1 Train Model from sklearn.linear_model import LinearRegression from dmba import adjusted_r2_score lr = LinearRegression() lr.fit(X_train, y_train) y_pred = lr.predict(X_test) y_pred = pd.DataFrame(y_pred, columns=[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]).astype(int) print(y_pred) print(adjusted_r2_score(y_test, y_pred, lr)) casual registered cnt 0 37 249 286 1 42 344 386 2 48 259 307 3 19 360 380 4 -9 -88 -98 ... ... ... ... 4340 6 205 211 4341 6 109 115 4342 32 226 259 4343 52 93 146 4344 -5 142 137 0.5979873068757086 Because number of people cannot be negative, we change some data so that the result make sense. for i in range(y_pred.shape[0]): if y_pred[&quot;casual&quot;].iloc[i] &lt; 0: y_pred[&quot;casual&quot;].iloc[i] = 0 if y_pred[&quot;registered&quot;].iloc[i] &lt; 0: y_pred[&quot;registered&quot;].iloc[i] = 0 y_pred[&quot;cnt&quot;].iloc[i] = y_pred[&quot;casual&quot;].iloc[i] + y_pred[&quot;registered&quot;].iloc[i] print(y_pred) print(adjusted_r2_score(y_test, y_pred, lr)) casual registered cnt 0 37 249 286 1 42 344 386 2 48 259 307 3 19 360 380 4 0 0 0 ... ... ... ... 4340 6 205 211 4341 6 109 115 4342 32 226 259 4343 52 93 146 4344 0 142 142 0.6153434162490663 ","link":"https://qingqiuzhang.github.io/bike-sharing-dadaset-analytics/"},{"title":"Bike Sharing Dataset Analytics (1) -- Daily Data","content":"This article aims to predict the count of casual users (feature casual), count of registered users (feature registered), and the total count of both causal and registered users (feature cnt) using the multi-regression model. You can go to this webpage to find the source data day.csv and also you can find Bike Sharing Dataset Analytics -- Daily Data.py here. Exploring the data import pandas as pd day = pd.read_csv(&quot;day.csv&quot;) day.head() instant dteday season yr mnth holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 6 0 2 0.344167 0.363625 0.805833 0.160446 331 654 985 1 2 2011-01-02 1 0 1 0 0 0 2 0.363478 0.353739 0.696087 0.248539 131 670 801 2 3 2011-01-03 1 0 1 0 1 1 1 0.196364 0.189405 0.437273 0.248309 120 1229 1349 3 4 2011-01-04 1 0 1 0 2 1 1 0.200000 0.212122 0.590435 0.160296 108 1454 1562 4 5 2011-01-05 1 0 1 0 3 1 1 0.226957 0.229270 0.436957 0.186900 82 1518 1600 Visualization import matplotlib.pyplot as plt import seaborn as sns result = day[[&quot;mnth&quot;, &quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].groupby([&quot;mnth&quot;]).mean() result = ( result.stack() .reset_index() .set_index(&quot;mnth&quot;) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people per day&quot;}) ) cat = [&quot;season&quot;, &quot;yr&quot;, &quot;holiday&quot;, &quot;weekday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;] sns.set(style=&quot;ticks&quot;, palette=&quot;pastel&quot;) f, axes = plt.subplots(3, 3, sharey=False, figsize=(15, 12)) ax = plt.subplot2grid((3, 3), (0, 0), colspan=3) sns.barplot(x=result.index, y=&quot;people per day&quot;, data=result, hue=&quot;cat&quot;, ax=ax) for ind, val in enumerate(cat): result = round(day[[val, &quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].groupby([val]).mean()) result = ( result.stack() .reset_index() .set_index(val) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people per day&quot;}) ) sns.barplot( x = result.index, y = &quot;people per day&quot;, data = result, hue = &quot;cat&quot;, ax = axes[ind // 3 + 1, ind % 3], ) f.tight_layout(pad=3.0) plt.show() result = day[[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].set_index(day[&quot;mnth&quot;]) result = ( result.stack() .reset_index() .set_index(&quot;mnth&quot;) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people&quot;}) ) f, axes = plt.subplots(3, 3, sharey=False, figsize=(20, 12)) ax = plt.subplot2grid((3, 3), (0, 0), colspan=3) sns.violinplot(x=result.index, y=&quot;people&quot;, hue=&quot;cat&quot;, data=result, cut=0, ax=ax) for ind, val in enumerate(cat): result = day[[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].set_index(day[val]) result = ( result.stack() .reset_index() .set_index(val) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people&quot;}) ) sns.violinplot( x = result.index, y = &quot;people&quot;, hue = &quot;cat&quot;, data = result, cut = 0, ax = axes[ind // 3 + 1, ind % 3], ) f.tight_layout(pad=3.0) plt.show() The number of people who rental a bike every year is increasing, which indicates that riding bicycle is getting popular. This trend could be got if we fit a model on date and year. But here, since we don't have data of enough years, so we maily focus on the other features in our analysis. Linear Regression Split Data from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler day[cat] = day[cat].apply(lambda x: x.astype(&quot;category&quot;)) dummies = pd.get_dummies(day[cat], drop_first=True) print(dummies.shape) conti_predictors = [&quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;] print(day[conti_predictors].shape) X = pd.concat([day[conti_predictors], dummies], axis=1) y = day.iloc[:, 13:] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) sc = StandardScaler() X_train[conti_predictors] = sc.fit_transform(X_train[conti_predictors]) X_test [conti_predictors] = sc.transform(X_test[conti_predictors]) (731, 24) (731, 4) Train Model from sklearn.linear_model import LinearRegression from dmba import adjusted_r2_score lr = LinearRegression() lr.fit(X_train, y_train) y_pred = lr.predict(X_test) y_pred = pd.DataFrame(y_pred, columns=[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]).astype(int) print(y_pred) print(adjusted_r2_score(y_test, y_pred, lr)) casual registered cnt 0 1119 5058 6178 1 886 3975 4861 2 961 5148 6109 3 865 5510 6375 4 -94 2389 2294 ... ... ... ... 178 317 2450 2768 179 -5 2653 2647 180 1252 4690 5942 181 465 2813 3278 182 482 3775 4258 0.5664326431246609 Because number of people cannot be negative, we change some data so that the result make sense. for i in range(y_pred.shape[0]): if y_pred[&quot;casual&quot;].iloc[i] &lt; 0: y_pred[&quot;casual&quot;].iloc[i] = 0 if y_pred[&quot;registered&quot;].iloc[i] &lt; 0: y_pred[&quot;registered&quot;].iloc[i] = 0 y_pred[&quot;cnt&quot;].iloc[i] = y_pred[&quot;casual&quot;].iloc[i] + y_pred[&quot;registered&quot;].iloc[i] print(y_pred) print(adjusted_r2_score(y_test, y_pred, lr)) casual registered cnt 0 1119 5058 6178 1 886 3975 4861 2 961 5148 6109 3 865 5510 6375 4 0 2389 2389 ... ... ... ... 178 317 2450 2768 179 0 2653 2653 180 1252 4690 5942 181 465 2813 3278 182 482 3775 4258 0.5761640858277055","link":"https://qingqiuzhang.github.io/bike-sharing-dataset-analytics/"},{"title":"National Center for Education Dataset Project","content":"The dataset comes from the National Center for Education Statistics. In this article, I will demonstrate some data cleaning and dataframe manipulation techniques. Hope this helps! For everything you may need, visit: Source Data nces-ed-attainment.csv National Center for Education.py The original dataset is titled: Percentage of persons 25 to 29 years old with selected levels of educational attainment, by race/ethnicity and sex: Selected years, 1920 through 2018. The cleaned version has columns for Year, Sex, Educational Attainment, and race/ethnicity categories considered in the dataset. Note that not all columns will have data starting at 1920. Exploring the data # packages needed import pandas as pd import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt df = pd.read_csv('nces-ed-attainment.csv') df.head() Year Sex Min degree Total White Black Hispanic Asian Pacific Islander American Indian/Alaska Native Two or more races 0 1920 A high school --- 22.0 6.3 --- --- --- --- --- 1 1940 A high school 38.1 41.2 12.3 --- --- --- --- --- 2 1950 A high school 52.8 56.3 23.6 --- --- --- --- --- 3 1960 A high school 60.7 63.7 38.6 --- --- --- --- --- 4 1970 A high school 75.4 77.8 58.4 --- --- --- --- --- There are so many \"---\" in dataset. We should convert it into null values for further manipulation. df = df.replace('---', np.nan) df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Year 214 non-null int64 1 Sex 214 non-null object 2 Min degree 214 non-null object 3 Total 214 non-null object 4 White 214 non-null float64 5 Black 214 non-null float64 6 Hispanic 214 non-null object 7 Asian 214 non-null object 8 Pacific Islander 214 non-null object 9 American Indian/Alaska Native 214 non-null object 10 Two or more races 214 non-null object dtypes: float64(2), int64(1), object(8) memory usage: 18.5+ KB Several columns have wrong type. race = [ &quot;Total&quot;, &quot;Hispanic&quot;, &quot;Asian&quot;, &quot;Pacific Islander&quot;, &quot;American Indian/Alaska Native&quot;, &quot;Two or more races&quot;, ] df[race] = df[race].apply(pd.to_numeric) df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Year 214 non-null int64 1 Sex 214 non-null object 2 Min degree 214 non-null object 3 Total 212 non-null float64 4 White 214 non-null float64 5 Black 214 non-null float64 6 Hispanic 204 non-null float64 7 Asian 168 non-null float64 8 Pacific Islander 93 non-null float64 9 American Indian/Alaska Native 131 non-null float64 10 Two or more races 157 non-null float64 dtypes: float64(8), int64(1), object(2) memory usage: 18.5+ KB Have a look at some indicators of each column. df.describe(include=&quot;all&quot;).T count unique top freq mean std min 25% 50% 75% max Year 214.0 NaN NaN NaN 2005.48 15.6 1920.0 2005.0 2010.0 2014.0 2018.0 Sex 214 3 A 78 NaN NaN NaN NaN NaN NaN NaN Min degree 214 4 high school 59 NaN NaN NaN NaN NaN NaN NaN Total 212.0 NaN NaN NaN 42.77 30.15 4.1 22.12 36.1 84.4 94.0 White 214.0 NaN NaN NaN 47.21 31.32 4.5 22.3 42.65 85.9 96.4 Black 214.0 NaN NaN NaN 35.19 32.36 1.1 10.12 23.2 70.62 93.5 Hispanic 204.0 NaN NaN NaN 27.68 26.85 0.6 7.6 16.6 57.02 87.2 Asian 168.0 NaN NaN NaN 62.02 27.03 15.0 46.55 66.0 81.05 98.5 Pacific Islander 93.0 NaN NaN NaN 50.79 33.24 10.0 24.0 34.0 90.7 100.0 American Indian/Alaska Native 131.0 NaN NaN NaN 42.67 32.1 2.1 16.55 24.4 83.05 95.1 Two or more races 157.0 NaN NaN NaN 44.46 31.58 2.9 25.7 34.7 87.9 98.2 Visulization Correlation Matrix Heatmap cormat = df[race].corr().round(2) sns.heatmap(cormat, annot=True) Enrollment Date Distribution f, axes = plt.subplots(4, 2, sharey=False, figsize=(15, 12)) for ind, val in enumerate(race): sns.histplot( data=df, x=val, hue=&quot;Min degree&quot;, label=&quot;100% Equities&quot;, kde=True, stat=&quot;density&quot;, linewidth=0, ax=axes[ind // 2, ind % 2], bins=200, ).set(title=val) f.tight_layout(pad=3.0) plt.show() For Black, Hispanic, Pacific Islander people and people with two or more races, the enrollment rate is low for most of the past time. For Asian and White people, the overall enrollment rate is high for most of the past time. f, axes = plt.subplots(4, 2, sharey=False, figsize=(14, 14)) for ind, val in enumerate(race): sns.lineplot( data=df, x=&quot;Year&quot;, y=val, hue=&quot;Min degree&quot;, ci=None, ax=axes[ind // 2, ind % 2] ) The Black and White people has low enrollment rate at the 1920s to 1970s, but the enrollments increased dramatically. For Black, White and Hispanic people, the enrollment rates for the four degrees has been increasing overtime. The enrollment rates of the four degrees keeps quite static since 2006 for Asian, Pacific Islander, American Indian/Alaska Native and people with two or more races. Questions Q1 What are the percent of different degrees completed for a given year range and sex? Parameter arguments are as follows: two year arguments, and a value for sex (’A’, ’F’, or ’M’). Function should return all rows of the data which match the given sex, and have data between the given years (inclusive for the start, exclusive for the end). If no data is found for the parameters, return Python keyword None. def completion_bet_years(dataframe, year1, year2, sex): &quot;&quot;&quot; Return percent of different degrees completed between year1 and year2 for Sex==sex. Args: dataframe (DataFrame): A dataframe containing the needed data year1 (int) : Year number: the earlier one year2 (int) : Year number: the later one sex (str) : Gender Returns: DataFrame: The percent of degrees completed between year1 and year2 for Sex==sex. &quot;&quot;&quot; result = dataframe.loc[ (dataframe[&quot;Sex&quot;] == sex) &amp; (dataframe[&quot;Year&quot;] &gt;= year1) &amp; (dataframe[&quot;Year&quot;] &lt; year2) ] if result.shape[0] == 0: return None else: return result print(completion_bet_years(df, 1920, 1941, &quot;A&quot;)) Year Sex Min degree Total White Black Hispanic Asian \\ 0 1920 A high school NaN 22.0 6.3 NaN NaN 1 1940 A high school 38.1 41.2 12.3 NaN NaN 39 1920 A bachelor's NaN 4.5 1.2 NaN NaN 40 1940 A bachelor's 5.9 6.4 1.6 NaN NaN Pacific Islander American Indian/Alaska Native Two or more races 0 NaN NaN NaN 1 NaN NaN NaN 39 NaN NaN NaN 40 NaN NaN NaN Q2 What were the percentages for women vs men having earned a Bachelor’s Degree in a given year? Parameter list argument is the year in question and return the percentages as a tuple: (% for men, % for women) def compare_bachelors_in_year(dataframe, year): &quot;&quot;&quot; Return the percentages for women vs men having earned a Bachelor’s Degree in Year==year Args: df (Dataframe): A dataframe containing the needed data year (int) : The year in which you want to know the info Returns: tuple: A tuple returns the percentage of Bachelor’s Degree male and femal in Year==year. &quot;&quot;&quot; women = dataframe[ (dataframe[&quot;Sex&quot;] == &quot;F&quot;) &amp; (dataframe[&quot;Min degree&quot;] == &quot;bachelor's&quot;) &amp; (dataframe[&quot;Year&quot;] == year) ].iloc[0][&quot;Total&quot;] men = dataframe[ (dataframe[&quot;Sex&quot;] == &quot;M&quot;) &amp; (dataframe[&quot;Min degree&quot;] == &quot;bachelor's&quot;) &amp; (dataframe[&quot;Year&quot;] == year) ].iloc[0][&quot;Total&quot;] return (f&quot;{men} % for men&quot;, f&quot;{women} % for women&quot;) compare_bachelors_in_year(df, 2010) ('27.8 % for men', '35.7 % for women') Q3 What were the two most commonly awarded levels of educational attainment awarded between 2000-2010 (inclusive)? Use the mean percent over the years to compare the education levels. Return a list as follows: [#1 level, mean % of #1 level, #2 level, mean % of #2 level]. def top_2_2000s(dataframe): &quot;&quot;&quot;Return the two most common educational attainment between 2000-2010. Args: DataFrame (DataFrame): A dataframe containing the needed data Returns: list: A list returns the two most common educational attainment between 2000-2010. &quot;&quot;&quot; df_2000s = dataframe.loc[ (dataframe[&quot;Year&quot;] &gt;= 2000) &amp; (dataframe[&quot;Year&quot;] &lt;= 2010) &amp; (dataframe[&quot;Sex&quot;] == &quot;A&quot;) ][[&quot;Total&quot;, &quot;Min degree&quot;]] mean_percent_edu = ( df_2000s.groupby(&quot;Min degree&quot;).mean().sort_values(by=&quot;Total&quot;, ascending=False) ) level1 = mean_percent_edu.iloc[0][&quot;Total&quot;].round(2) level2 = mean_percent_edu.iloc[1][&quot;Total&quot;].round(2) edu1 = mean_percent_edu.index[0] edu2 = mean_percent_edu.index[1] return [f&quot;#1 level, {level1} % of {edu1}&quot;, f&quot;#2 level, {level2} % of {edu2}&quot;] top_2_2000s(df) ['#1 level, 87.557 % of high school', \"#2 level, 38.757 % of associate's\"] ","link":"https://qingqiuzhang.github.io/national-center-for-education-dataset-project/"}]}