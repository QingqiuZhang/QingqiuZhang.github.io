{"posts":[{"title":"Bike Sharing Dadaset Analytics (2) -- Hourly Data","content":"kl ","link":"https://qingqiuzhang.github.io/bike-sharing-dadaset-analytics/"},{"title":"Iris Dataset Analytics","content":"The Iris Data set was created by R.A. Fisher and is perhaps the best known data set to be found in the pattern recognition literature. Fisherâ€™s paper is a classic in the field and is referenced frequently to this day. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. Predicted attribute: class of iris plant. This article aims to classify this dataset utilizing the following models: K Nearest Neighbors (KNN), Naive Bayes and Logistic Regression. You can find the data source and Iris Dataset Analytics.py here. Exploring Data import pandas as pd import numpy as np df = pd.read_csv( &quot;/Users/qingqiuzhang/Desktop/iris_dataset.csv&quot;, header = None, names = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;, &quot;class&quot;], ) df.head() sepal length sepal width petal length petal width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa Calculate some indicators of features. df.describe() sepal length sepal width petal length petal width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 ## Visualization import matplotlib.pyplot as plt import seaborn as sns features = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;] sns.set(style=&quot;ticks&quot;, palette=&quot;pastel&quot;) f, axes = plt.subplots(2, 2, sharey=False, figsize=(14, 14)) for ind, val in enumerate(features): sns.violinplot(x=&quot;class&quot;, y=val, data=df, ax=axes[ind // 2, ind % 2]).set( title = &quot;Sepal Length&quot; ) plt.show() sns.pairplot(df, hue=&quot;class&quot;) Train and evaluate three models using cross-validation K-Nearest Neighbors Classifier from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import ShuffleSplit from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV NUM = 200 X = df.drop([&quot;class&quot;], axis=1) y = df[&quot;class&quot;] shuffle = ShuffleSplit(n_splits=NUM, test_size=0.25, random_state=10) results = [] klist = np.arange(1, 21, 1) FullModel = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;knn&quot;, KNeighborsClassifier())]) param_grid = {&quot;knn__n_neighbors&quot;: klist} grid_search = GridSearchCV( FullModel, param_grid, scoring = &quot;accuracy&quot;, cv = shuffle, return_train_score = True, n_jobs = -1, ) grid_search.fit(X, y) results = pd.DataFrame(grid_search.cv_results_) print( results[ [ &quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_knn__n_neighbors&quot;, ] ] ) rank_test_score mean_train_score mean_test_score param_knn__n_neighbors 0 18 1.000000 0.943947 1 1 20 0.970268 0.937368 2 2 17 0.959018 0.945132 3 3 15 0.958214 0.945789 4 4 12 0.963304 0.949737 5 5 9 0.963080 0.952105 6 6 6 0.966741 0.954605 7 7 3 0.962500 0.955395 8 8 7 0.963482 0.954474 9 9 5 0.963571 0.955000 10 10 2 0.965536 0.956447 11 11 1 0.965982 0.958553 12 12 4 0.963973 0.955132 13 13 8 0.963661 0.954079 14 14 11 0.961205 0.950000 15 15 10 0.959107 0.950658 16 16 14 0.957143 0.946711 17 17 13 0.956205 0.947895 18 18 16 0.955223 0.945526 19 19 19 0.953036 0.942895 20 # Plot Accuracy vs K fig, ax = plt.subplots() ax.plot( results[&quot;param_knn__n_neighbors&quot;], results[&quot;mean_test_score&quot;], label=&quot;test accuracy&quot; ) ax.set_xlim(15, 0) # reverse x; from simple model to complex model # (complex model tries hard to sort of figure out all sorts of details in the data) ax.set_ylabel(&quot;Accuracy&quot;) ax.set_xlabel(&quot;n_neighbors&quot;) ax.grid() ax.legend() Naive Bayes Classifier from sklearn.naive_bayes import MultinomialNB from sklearn.preprocessing import MinMaxScaler features = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;] df1 = df.copy() for column in features: df1[column] = [round(i - np.min(df1[column])) for i in df1[column]] df1[column] = df1[column].astype(&quot;category&quot;) X = pd.get_dummies(df1[features]) # Calculate mean test accuracy alphas = [0.01, 0.1, 1.0, 5.0, 10.0, 15.0, 20.0, 35.0, 50.0] FullModel = Pipeline([(&quot;scaler&quot;, MinMaxScaler()), (&quot;mnb&quot;, MultinomialNB())]) param_grid = {&quot;mnb__alpha&quot;: alphas} grid_search = GridSearchCV( FullModel, param_grid, scoring = &quot;accuracy&quot;, cv = shuffle, return_train_score = True, n_jobs = -1, ) grid_search.fit(X, y) results = pd.DataFrame(grid_search.cv_results_) print( results[ [&quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_mnb__alpha&quot;] ] ) rank_test_score mean_train_score mean_test_score param_mnb__alpha 0 9 0.942054 0.931711 0.01 1 8 0.942187 0.932105 0.1 2 7 0.944420 0.935789 1.0 3 5 0.947321 0.942237 5.0 4 3 0.946830 0.945132 10.0 5 1 0.946786 0.946184 15.0 6 2 0.946741 0.946053 20.0 7 4 0.945312 0.944211 35.0 8 6 0.943080 0.941053 50.0 Logistic Regression from sklearn.linear_model import LogisticRegression df2 = df.copy() X = df2.iloc[:, 0:4].values y = df2.iloc[:, 4].values # Calculate mean test accuracy Clist = [0.01, 0.1, 1.0, 5.0, 10.0, 20.0, 50.0, 100.0, 150.0, 200.0] FullModel = Pipeline( [ (&quot;scaler&quot;, StandardScaler()), (&quot;lr&quot;, LogisticRegression(penalty=&quot;l2&quot;, solver=&quot;lbfgs&quot;, multi_class=&quot;auto&quot;)), ] ) param_grid = {&quot;lr__C&quot;: Clist} grid_search = GridSearchCV( FullModel, param_grid, scoring = &quot;accuracy&quot;, cv = shuffle, return_train_score = True, n_jobs = -1, ) grid_search.fit(X, y) results = pd.DataFrame(grid_search.cv_results_) print( results[[&quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_lr__C&quot;]] ) rank_test_score mean_train_score mean_test_score param_lr__C 0 10 0.852321 0.834342 0.01 1 9 0.919777 0.908684 0.1 2 8 0.969509 0.960132 1.0 3 7 0.979420 0.968026 5.0 4 4 0.981295 0.969342 10.0 5 3 0.982857 0.969737 20.0 6 1 0.984598 0.970263 50.0 7 2 0.986205 0.969737 100.0 8 5 0.986786 0.969211 150.0 9 6 0.987009 0.968684 200.0 Conclusion For the current running, Logistic Regression has the highest test accuracy followed by KNN Classifier. Naive Bayes Classifier has the lowest test accuracy. ","link":"https://qingqiuzhang.github.io/iris-data-set-analytics/"},{"title":"Bike Sharing Dataset Analytics (1) -- Daily Data","content":"This article aims to predict the count of casual users (feature casual), count of registered users (feature registered), and the total count of both causal and registered users (feature cnt) using the multi-regression model. Exploring the data import pandas as pd day = pd.read_csv(&quot;day.csv&quot;) day.head() instant dteday season yr mnth holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 6 0 2 0.344167 0.363625 0.805833 0.160446 331 654 985 1 2 2011-01-02 1 0 1 0 0 0 2 0.363478 0.353739 0.696087 0.248539 131 670 801 2 3 2011-01-03 1 0 1 0 1 1 1 0.196364 0.189405 0.437273 0.248309 120 1229 1349 3 4 2011-01-04 1 0 1 0 2 1 1 0.200000 0.212122 0.590435 0.160296 108 1454 1562 4 5 2011-01-05 1 0 1 0 3 1 1 0.226957 0.229270 0.436957 0.186900 82 1518 1600 Visualization import matplotlib.pyplot as plt import seaborn as sns result = day[[&quot;mnth&quot;, &quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].groupby([&quot;mnth&quot;]).mean() result = ( result.stack() .reset_index() .set_index(&quot;mnth&quot;) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people per day&quot;}) ) cat = [&quot;season&quot;, &quot;yr&quot;, &quot;holiday&quot;, &quot;weekday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;] sns.set(style=&quot;ticks&quot;, palette=&quot;pastel&quot;) f, axes = plt.subplots(3, 3, sharey=False, figsize=(15, 12)) ax = plt.subplot2grid((3, 3), (0, 0), colspan=3) sns.barplot(x=result.index, y=&quot;people per day&quot;, data=result, hue=&quot;cat&quot;, ax=ax) for ind, val in enumerate(cat): result = round(day[[val, &quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].groupby([val]).mean()) result = ( result.stack() .reset_index() .set_index(val) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people per day&quot;}) ) sns.barplot( x = result.index, y = &quot;people per day&quot;, data = result, hue = &quot;cat&quot;, ax = axes[ind // 3 + 1, ind % 3], ) f.tight_layout(pad=3.0) plt.show() result = day[[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].set_index(day[&quot;mnth&quot;]) result = ( result.stack() .reset_index() .set_index(&quot;mnth&quot;) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people&quot;}) ) f, axes = plt.subplots(3, 3, sharey=False, figsize=(20, 12)) ax = plt.subplot2grid((3, 3), (0, 0), colspan=3) sns.violinplot(x=result.index, y=&quot;people&quot;, hue=&quot;cat&quot;, data=result, cut=0, ax=ax) for ind, val in enumerate(cat): result = day[[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].set_index(day[val]) result = ( result.stack() .reset_index() .set_index(val) .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people&quot;}) ) sns.violinplot( x = result.index, y = &quot;people&quot;, hue = &quot;cat&quot;, data = result, cut = 0, ax = axes[ind // 3 + 1, ind % 3], ) f.tight_layout(pad=3.0) plt.show() The number of people who rental a bike every year is increasing, which indicates that riding bicycle is getting popular. This trend could be got if we fit a model on date and year. But here, since we don't have data of enough years, so we maily focus on the other features in our analysis. Linear Regression Split Data from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler day[cat] = day[cat].apply(lambda x: x.astype(&quot;category&quot;)) dummies = pd.get_dummies(day[cat], drop_first=True) print(dummies.shape) conti_predictors = [&quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;] print(day[conti_predictors].shape) X = pd.concat([day[conti_predictors], dummies], axis=1) y = day.iloc[:, 13:] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) sc = StandardScaler() X_train[conti_predictors] = sc.fit_transform(X_train[conti_predictors]) X_test [conti_predictors] = sc.transform(X_test[conti_predictors]) (731, 24) (731, 4) Train Model from sklearn.linear_model import LinearRegression from dmba import adjusted_r2_score lr = LinearRegression() lr.fit(X_train, y_train) y_pred = lr.predict(X_test) y_pred = pd.DataFrame(y_pred, columns=[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]).astype(int) print(y_pred) print(adjusted_r2_score(y_test, y_pred, lr)) casual registered cnt 0 1119 5058 6178 1 886 3975 4861 2 961 5148 6109 3 865 5510 6375 4 -94 2389 2294 ... ... ... ... 178 317 2450 2768 179 -5 2653 2647 180 1252 4690 5942 181 465 2813 3278 182 482 3775 4258 0.5664326431246609 Because number of people cannot be negative, we change some data so that the result make sense. for i in range(y_pred.shape[0]): if y_pred[&quot;casual&quot;].iloc[i] &lt; 0: y_pred[&quot;casual&quot;].iloc[i] = 0 if y_pred[&quot;registered&quot;].iloc[i] &lt; 0: y_pred[&quot;registered&quot;].iloc[i] = 0 y_pred[&quot;cnt&quot;].iloc[i] = y_pred[&quot;casual&quot;].iloc[i] + y_pred[&quot;registered&quot;].iloc[i] print(y_pred) print(adjusted_r2_score(y_test, y_pred, lr)) casual registered cnt 0 1119 5058 6178 1 886 3975 4861 2 961 5148 6109 3 865 5510 6375 4 0 2389 2389 ... ... ... ... 178 317 2450 2768 179 0 2653 2653 180 1252 4690 5942 181 465 2813 3278 182 482 3775 4258 0.5761640858277055","link":"https://qingqiuzhang.github.io/bike-sharing-dataset-analytics/"},{"title":"National Center for Education Dataset Project","content":"The dataset you will be processing comes from the National Center for Education Statistics. You can find the original dataset here. This data has been cleaned a bit and you can use the the provided file nces-ed-attainment.csv. You can find the whole python file National Center for Education.py here. The original dataset is titled: Percentage of persons 25 to 29 years old with selected levels of educational attainment, by race/ethnicity and sex: Selected years, 1920 through 2018. The cleaned version you will be working with has columns for Year, Sex, Educational Attainment, and race/ethnicity categories considered in the dataset. Note that not all columns will have data starting at 1920. Exploring the data # packages needed import pandas as pd import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt df = pd.read_csv('nces-ed-attainment.csv') df.head() Year Sex Min degree Total White Black Hispanic Asian Pacific Islander American Indian/Alaska Native Two or more races 0 1920 A high school --- 22.0 6.3 --- --- --- --- --- 1 1940 A high school 38.1 41.2 12.3 --- --- --- --- --- 2 1950 A high school 52.8 56.3 23.6 --- --- --- --- --- 3 1960 A high school 60.7 63.7 38.6 --- --- --- --- --- 4 1970 A high school 75.4 77.8 58.4 --- --- --- --- --- There are so many \"---\" in dataset. We should convert it into null values for further manipulation. df = df.replace('---', np.nan) df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Year 214 non-null int64 1 Sex 214 non-null object 2 Min degree 214 non-null object 3 Total 214 non-null object 4 White 214 non-null float64 5 Black 214 non-null float64 6 Hispanic 214 non-null object 7 Asian 214 non-null object 8 Pacific Islander 214 non-null object 9 American Indian/Alaska Native 214 non-null object 10 Two or more races 214 non-null object dtypes: float64(2), int64(1), object(8) memory usage: 18.5+ KB Several columns have wrong type. race = [ &quot;Total&quot;, &quot;Hispanic&quot;, &quot;Asian&quot;, &quot;Pacific Islander&quot;, &quot;American Indian/Alaska Native&quot;, &quot;Two or more races&quot;, ] df[race] = df[race].apply(pd.to_numeric) df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Year 214 non-null int64 1 Sex 214 non-null object 2 Min degree 214 non-null object 3 Total 212 non-null float64 4 White 214 non-null float64 5 Black 214 non-null float64 6 Hispanic 204 non-null float64 7 Asian 168 non-null float64 8 Pacific Islander 93 non-null float64 9 American Indian/Alaska Native 131 non-null float64 10 Two or more races 157 non-null float64 dtypes: float64(8), int64(1), object(2) memory usage: 18.5+ KB Have a look at some indicators of each column. df.describe(include=&quot;all&quot;).T count unique top freq mean std min 25% 50% 75% max Year 214.0 NaN NaN NaN 2005.48 15.6 1920.0 2005.0 2010.0 2014.0 2018.0 Sex 214 3 A 78 NaN NaN NaN NaN NaN NaN NaN Min degree 214 4 high school 59 NaN NaN NaN NaN NaN NaN NaN Total 212.0 NaN NaN NaN 42.77 30.15 4.1 22.12 36.1 84.4 94.0 White 214.0 NaN NaN NaN 47.21 31.32 4.5 22.3 42.65 85.9 96.4 Black 214.0 NaN NaN NaN 35.19 32.36 1.1 10.12 23.2 70.62 93.5 Hispanic 204.0 NaN NaN NaN 27.68 26.85 0.6 7.6 16.6 57.02 87.2 Asian 168.0 NaN NaN NaN 62.02 27.03 15.0 46.55 66.0 81.05 98.5 Pacific Islander 93.0 NaN NaN NaN 50.79 33.24 10.0 24.0 34.0 90.7 100.0 American Indian/Alaska Native 131.0 NaN NaN NaN 42.67 32.1 2.1 16.55 24.4 83.05 95.1 Two or more races 157.0 NaN NaN NaN 44.46 31.58 2.9 25.7 34.7 87.9 98.2 Visulization Correlation Matrix Heatmap cormat = df[race].corr().round(2) sns.heatmap(cormat, annot=True) Enrollment Date Distribution f, axes = plt.subplots(4, 2, sharey=False, figsize=(15, 12)) for ind, val in enumerate(race): sns.histplot( data=df, x=val, hue=&quot;Min degree&quot;, label=&quot;100% Equities&quot;, kde=True, stat=&quot;density&quot;, linewidth=0, ax=axes[ind // 2, ind % 2], bins=200, ).set(title=val) f.tight_layout(pad=3.0) plt.show() For Black, Hispanic, Pacific Islander people and people with two or more races, the enrollment rate is low for most of the past time. For Asian and White people, the overall enrollment rate is high for most of the past time. f, axes = plt.subplots(4, 2, sharey=False, figsize=(14, 14)) for ind, val in enumerate(race): sns.lineplot( data=df, x=&quot;Year&quot;, y=val, hue=&quot;Min degree&quot;, ci=None, ax=axes[ind // 2, ind % 2] ) The Black and White people has low enrollment rate at the 1920s to 1970s, but the enrollments increased dramatically. For Black, White and Hispanic people, the enrollment rates for the four degrees has been increasing overtime. The enrollment rates of the four degrees keeps quite static since 2006 for Asian, Pacific Islander, American Indian/Alaska Native and people with two or more races. Questions Q1 What are the percent of different degrees completed for a given year range and sex? Parameter arguments are as follows: two year arguments, and a value for sex (â€™Aâ€™, â€™Fâ€™, or â€™Mâ€™). Function should return all rows of the data which match the given sex, and have data between the given years (inclusive for the start, exclusive for the end). If no data is found for the parameters, return Python keyword None. def completion_bet_years(dataframe, year1, year2, sex): &quot;&quot;&quot; Return percent of different degrees completed between year1 and year2 for Sex==sex. Args: dataframe (DataFrame): A dataframe containing the needed data year1 (int) : Year number: the earlier one year2 (int) : Year number: the later one sex (str) : Gender Returns: DataFrame: The percent of degrees completed between year1 and year2 for Sex==sex. &quot;&quot;&quot; result = dataframe.loc[ (dataframe[&quot;Sex&quot;] == sex) &amp; (dataframe[&quot;Year&quot;] &gt;= year1) &amp; (dataframe[&quot;Year&quot;] &lt; year2) ] if result.shape[0] == 0: return None else: return result print(completion_bet_years(df, 1920, 1941, &quot;A&quot;)) Year Sex Min degree Total White Black Hispanic Asian \\ 0 1920 A high school NaN 22.0 6.3 NaN NaN 1 1940 A high school 38.1 41.2 12.3 NaN NaN 39 1920 A bachelor's NaN 4.5 1.2 NaN NaN 40 1940 A bachelor's 5.9 6.4 1.6 NaN NaN Pacific Islander American Indian/Alaska Native Two or more races 0 NaN NaN NaN 1 NaN NaN NaN 39 NaN NaN NaN 40 NaN NaN NaN Q2 What were the percentages for women vs men having earned a Bachelorâ€™s Degree in a given year? Parameter list argument is the year in question and return the percentages as a tuple: (% for men, % for women) def compare_bachelors_in_year(dataframe, year): &quot;&quot;&quot; Return the percentages for women vs men having earned a Bachelorâ€™s Degree in Year==year Args: df (Dataframe): A dataframe containing the needed data year (int) : The year in which you want to know the info Returns: tuple: A tuple returns the percentage of Bachelorâ€™s Degree male and femal in Year==year. &quot;&quot;&quot; women = dataframe[ (dataframe[&quot;Sex&quot;] == &quot;F&quot;) &amp; (dataframe[&quot;Min degree&quot;] == &quot;bachelor's&quot;) &amp; (dataframe[&quot;Year&quot;] == year) ].iloc[0][&quot;Total&quot;] men = dataframe[ (dataframe[&quot;Sex&quot;] == &quot;M&quot;) &amp; (dataframe[&quot;Min degree&quot;] == &quot;bachelor's&quot;) &amp; (dataframe[&quot;Year&quot;] == year) ].iloc[0][&quot;Total&quot;] return (f&quot;{men} % for men&quot;, f&quot;{women} % for women&quot;) compare_bachelors_in_year(df, 2010) ('27.8 % for men', '35.7 % for women') Q3 What were the two most commonly awarded levels of educational attainment awarded between 2000-2010 (inclusive)? Use the mean percent over the years to compare the education levels. Return a list as follows: [#1 level, mean % of #1 level, #2 level, mean % of #2 level]. def top_2_2000s(dataframe): &quot;&quot;&quot;Return the two most common educational attainment between 2000-2010. Args: DataFrame (DataFrame): A dataframe containing the needed data Returns: list: A list returns the two most common educational attainment between 2000-2010. &quot;&quot;&quot; df_2000s = dataframe.loc[ (dataframe[&quot;Year&quot;] &gt;= 2000) &amp; (dataframe[&quot;Year&quot;] &lt;= 2010) &amp; (dataframe[&quot;Sex&quot;] == &quot;A&quot;) ][[&quot;Total&quot;, &quot;Min degree&quot;]] mean_percent_edu = ( df_2000s.groupby(&quot;Min degree&quot;).mean().sort_values(by=&quot;Total&quot;, ascending=False) ) level1 = mean_percent_edu.iloc[0][&quot;Total&quot;].round(2) level2 = mean_percent_edu.iloc[1][&quot;Total&quot;].round(2) edu1 = mean_percent_edu.index[0] edu2 = mean_percent_edu.index[1] return [f&quot;#1 level, {level1} % of {edu1}&quot;, f&quot;#2 level, {level2} % of {edu2}&quot;] top_2_2000s(df) ['#1 level, 87.557 % of high school', \"#2 level, 38.757 % of associate's\"] ","link":"https://qingqiuzhang.github.io/national-center-for-education-dataset-project/"}]}