{"posts":[{"title":"National Center for Education Dataset Project","content":"The dataset you will be processing comes from the National Center for Education Statistics. You can find the original dataset here. This data has been cleaned a bit and you can use the the provided file nces-ed-attainment.csv. The original dataset is titled: Percentage of persons 25 to 29 years old with selected levels of educational attainment, by race/ethnicity and sex: Selected years, 1920 through 2018. The cleaned version you will be working with has columns for Year, Sex, Educational Attainment, and race/ethnicity categories considered in the dataset. Note that not all columns will have data starting at 1920. Exploring the data import pandas as pd df = pd.read_csv('nces-ed-attainment.csv') df.head() Year Sex Min degree Total White Black Hispanic Asian Pacific Islander American Indian/Alaska Native Two or more races 0 1920 A high school --- 22.0 6.3 --- --- --- --- --- 1 1940 A high school 38.1 41.2 12.3 --- --- --- --- --- 2 1950 A high school 52.8 56.3 23.6 --- --- --- --- --- 3 1960 A high school 60.7 63.7 38.6 --- --- --- --- --- 4 1970 A high school 75.4 77.8 58.4 --- --- --- --- --- There are so many \"---\" in dataset. We should convert it into null values for further manipulation. df = df.replace('---', np.nan) df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Year 214 non-null int64 1 Sex 214 non-null object 2 Min degree 214 non-null object 3 Total 214 non-null object 4 White 214 non-null float64 5 Black 214 non-null float64 6 Hispanic 214 non-null object 7 Asian 214 non-null object 8 Pacific Islander 214 non-null object 9 American Indian/Alaska Native 214 non-null object 10 Two or more races 214 non-null object dtypes: float64(2), int64(1), object(8) memory usage: 18.5+ KB Several columns have wrong type. race = ['Total', 'Hispanic', 'Asian', 'Pacific Islander', 'American Indian/Alaska Native', 'Two or more races'] df[race] = df[race].apply(pd.to_numeric) df.describe(include = 'all') df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 214 entries, 0 to 213 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Year 214 non-null int64 1 Sex 214 non-null object 2 Min degree 214 non-null object 3 Total 212 non-null float64 4 White 214 non-null float64 5 Black 214 non-null float64 6 Hispanic 204 non-null float64 7 Asian 168 non-null float64 8 Pacific Islander 93 non-null float64 9 American Indian/Alaska Native 131 non-null float64 10 Two or more races 157 non-null float64 dtypes: float64(8), int64(1), object(2) memory usage: 18.5+ KB Have a look at some indicators of each column. df.describe(include = 'all').T count unique top freq mean std min 25% 50% 75% max Year 214.0 NaN NaN NaN 2005.48 15.6 1920.0 2005.0 2010.0 2014.0 2018.0 Sex 214 3 A 78 NaN NaN NaN NaN NaN NaN NaN Min degree 214 4 high school 59 NaN NaN NaN NaN NaN NaN NaN Total 212.0 NaN NaN NaN 42.77 30.15 4.1 22.12 36.1 84.4 94.0 White 214.0 NaN NaN NaN 47.21 31.32 4.5 22.3 42.65 85.9 96.4 Black 214.0 NaN NaN NaN 35.19 32.36 1.1 10.12 23.2 70.62 93.5 Hispanic 204.0 NaN NaN NaN 27.68 26.85 0.6 7.6 16.6 57.02 87.2 Asian 168.0 NaN NaN NaN 62.02 27.03 15.0 46.55 66.0 81.05 98.5 Pacific Islander 93.0 NaN NaN NaN 50.79 33.24 10.0 24.0 34.0 90.7 100.0 American Indian/Alaska Native 131.0 NaN NaN NaN 42.67 32.1 2.1 16.55 24.4 83.05 95.1 Two or more races 157.0 NaN NaN NaN 44.46 31.58 2.9 25.7 34.7 87.9 98.2 # Visulization ## Computing multi-variate statistics of dataset (Correlation Matrix Heatmap) import seaborn as sns cormat = df[race].corr().round(2) sns.heatmap(cormat, annot=True) Plot the enrollment rate distribution for different min degree of people with different races f, axes = plt.subplots(4, 2, sharey=False, figsize=(15, 12)) for ind, val in enumerate(race): sns.histplot(data=df, x=val, hue='Min degree', label=&quot;100% Equities&quot;, kde=True, stat=&quot;density&quot;, linewidth=0, ax=axes[ind//2,ind%2], bins=200).set(title=val) f.tight_layout(pad=3.0) plt.show() For Black, Hispanic, Pacific Islander people and people with two or more races, the enrollment rate is low for most of the past time. For Asian and White people, the overall enrollment rate is high for most of the past time. f, axes = plt.subplots(4, 2, sharey=False, figsize=(14, 14)) sns.lineplot(data = df, x = &quot;Year&quot;, y = &quot;Total&quot;, hue='Min degree', ci=None, ax=axes[0,0]) sns.lineplot(data = df, x = &quot;Year&quot;, y = &quot;White&quot;, hue='Min degree', ci=None, ax=axes[0,1]) sns.lineplot(data = df, x = &quot;Year&quot;, y = &quot;Black&quot;, hue='Min degree', ci=None, ax=axes[1,0]) sns.lineplot(data = df, x = &quot;Year&quot;, y = &quot;Hispanic&quot;, hue='Min degree', ci=None, ax=axes[1,1]) sns.lineplot(data = df, x = &quot;Year&quot;, y = &quot;Asian&quot;, hue='Min degree', ci=None, ax=axes[2,0]) sns.lineplot(data = df, x = &quot;Year&quot;, y = &quot;Pacific Islander&quot;, hue='Min degree', ci=None, ax=axes[2,1]) sns.lineplot(data = df, x = &quot;Year&quot;, y = &quot;American Indian/Alaska Native&quot;, hue='Min degree', ci=None, ax=axes[3,0]) sns.lineplot(data = df, x = &quot;Year&quot;, y = &quot;Two or more races&quot;, hue='Min degree', ci=None, ax=axes[3,1]) The Black and White people has low enrollment rate at the 1920s to 1970s, but the enrollments increased dramatically. For Black, White and Hispanic people, the enrollment rates for the four degrees has been increasing overtime. The enrollment rates of the four degrees keeps quite static since 2006 for Asian, Pacific Islander, American Indian/Alaska Native and people with two or more races. Questions Q1 What are the percent of different degrees completed for a given year range and sex? Parameter arguments are as follows: two year arguments, and a value for sex (’A’, ’F’, or ’M’). Function should return all rows of the data which match the given sex, and have data between the given years (inclusive for the start, exclusive for the end). If no data is found for the parameters, return Python keyword None. def completion_bet_years(df1,year1,year2,sex): result = df1.loc[(df1['Sex']==sex) &amp; (df1['Year'] &gt;= year1) &amp; (df1['Year'] &lt; year2)] if result.shape[0] == 0: return None else: return result print(completion_bet_years(df,1920,1941,'A')) Year Sex Min degree Total White Black Hispanic Asian \\ 0 1920 A high school NaN 22.0 6.3 NaN NaN 1 1940 A high school 38.1 41.2 12.3 NaN NaN 39 1920 A bachelor's NaN 4.5 1.2 NaN NaN 40 1940 A bachelor's 5.9 6.4 1.6 NaN NaN Pacific Islander American Indian/Alaska Native Two or more races 0 NaN NaN NaN 1 NaN NaN NaN 39 NaN NaN NaN 40 NaN NaN NaN Q2 What were the percentages for women vs men having earned a Bachelor’s Degree in a given year? Parameter list argument is the year in question and return the percentages as a tuple: (% for men, % for women) def compare_bachelors_in_year(df, year): women = df[(df['Sex']=='F') &amp; (df['Min degree']==&quot;bachelor's&quot;) &amp; (df['Year']==year)].iloc[0]['Total'] men = df[(df['Sex']=='M') &amp; (df['Min degree']==&quot;bachelor's&quot;) &amp; (df['Year']==year)].iloc[0]['Total'] return ('{} % for men'.format(men),'{} % for women'.format(women)) compare_bachelors_in_year(df,2010) ('27.8 % for men', '35.7 % for women') Q3 What were the two most commonly awarded levels of educational attainment awarded between 2000-2010 (inclusive)? Use the mean percent over the years to compare the education levels. Return a list as follows: [#1 level, mean % of #1 level, #2 level, mean % of #2 level]. def top_2_2000s(df): df_2000s = df.loc[(df['Year']&gt;=2000) &amp; (df['Year']&lt;=2010) &amp; (df['Sex']=='A')][['Total','Min degree']] mean_percent_edu = df_2000s.groupby('Min degree').mean().sort_values(by='Total',ascending=False) level1 = mean_percent_edu.iloc[0]['Total'] level2 = mean_percent_edu.iloc[1]['Total'] edu1 = mean_percent_edu.index[0] edu2 = mean_percent_edu.index[1] return ['#1 level, {level:.3f} % of {edu}'.format(level=level1, edu=edu1), '#2 level, {level:.3f} % of {edu}'.format(level=level2, edu=edu2)] top_2_2000s(df) ['#1 level, 87.557 % of high school', \"#2 level, 38.757 % of associate's\"]","link":"https://qingqiuzhang.github.io/national-center-for-education-dataset-project/"},{"title":"Retail Data Anlytics (1) -- Data Preprocessing","content":"This is a thorough analysis of retail dataset from Kaggle. In this article, I will demonstrate how to an end-to-end machine learning project using diverse regression models. In case you need the source data, visit this webpage: Data Source The meaning of each column is stated as below: Store: the store number Date: the week Temperature: average temperature in the region Fuel_Price: cost of fuel in the region MarkDown1-5: anonymized data related to promotional markdowns. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA CPI: the consumer price index Unemployment: the unemployment rate ","link":"https://qingqiuzhang.github.io/retail-data-anlytics-1-data-preprocessing/"},{"title":"Diabetes Data Analytics (2) -- Model Training And Evaluating ","content":"In the previous article, I did data preprocessing. It is often seen as one of the most important step during a machine learning project. One must be patient with the data manimulation no matter how troublesome it is. In this article, we are finally exposed to the model traing procedure. I will demonstrate how to conduct different types of classification models and how to evalute their performance. In case you need the source data, visit this webpage: Data Source Model Training And Evaluating There are so many classification models. Some you may have heard or used while the others not. This articles covers almost all the classification models you may need as a beginner. I will show you how to conduct them and how different hyperparameters will affect the models. K-Nearest Neighbor Classifier Logistic Regression Ordinary Logistic Regression Ridge Lasss Combined Ridge and Lasso Naive Bayes Classifier Support Vector Machine Linear Nonlinear Decision Tree Random Forest ","link":"https://qingqiuzhang.github.io/diabetes-dataset-report-/"},{"title":"Diabetes Data Analytics (1) -- Data Preprocessing","content":"This is a thorough analysis of diabetes dataset from Kaggle. In this article, I will demonstrate how to an end-to-end machine learning project using diverse classification models. In case you need the source data, visit this webpage: Data Source What Can We Do With This Dataset? The observations in the dataset are all females from Pima Indian heritage who are greater than or equal to 21 years old. Data Preprocessing I cannot emphasize the importance of data preprocessing. It is the process of transforming raw data into more reasonable, useful and efficient format. It's a must and one of the most important step in a machine learing project. Without it our models will probably crash and won't be able to generate good results. There are several steps in data preprocessing: data cleaning, data reduction and data transformation. In the following paragraphs, I'll show you how to perform the steps one by one. Data Cleaning # packages needed import numpy as np import pandas as pd # load data data = pd.read_csv('diabetes.csv') data.head(10) Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 5 5 116 74 0 0 25.6 0.201 30 0 6 3 78 50 32 88 31.0 0.248 26 1 7 10 115 0 0 0 35.3 0.134 29 0 8 2 197 70 45 543 30.5 0.158 53 1 9 8 125 96 0 0 0.0 0.232 54 1 The meaning of each column is stated as below: Pregnancies: Number of times pregnant Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test BloodPressure: Diastolic blood pressure (mm Hg) SkinThickness: Triceps skin fold thickness (mm) Insulin: 2-Hour serum insulin (mu U/ml) BMI: Body mass index (weight in kg/(height in m)^2) DiabetesPedigreeFunction: Diabetes pedigree function Age: Age (years) Outcome: Class variable (0 or 1) Next, we need to examin whether there are null values in the dataset. data.isnull().sum() Pregnancies 0 Glucose 0 BloodPressure 0 SkinThickness 0 Insulin 0 BMI 0 DiabetesPedigreeFunction 0 Age 0 Outcome 0 dtype: int64 It seems there is no null values. Shall we pop the champagne now? If we further examine the data, we will find that the situation is not as good as we thought. data.describe().round(2) count mean std min 25% 50% 75% max Pregnancies 768.0 3.85 3.37 0.00 1.00 3.00 6.00 17.00 Glucose 768.0 120.89 31.97 0.00 99.00 117.00 140.25 199.00 BloodPressure 768.0 69.11 19.36 0.00 62.00 72.00 80.00 122.00 SkinThickness 768.0 20.54 15.95 0.00 0.00 23.00 32.00 99.00 Insulin 768.0 79.80 115.24 0.00 0.00 30.50 127.25 846.00 BMI 768.0 31.99 7.88 0.00 27.30 32.00 36.60 67.10 DiabetesPedigreeFunction 768.0 0.47 0.33 0.08 0.24 0.37 0.63 2.42 Age 768.0 33.24 11.76 21.00 24.00 29.00 41.00 81.00 Outcome 768.0 0.35 0.48 0.00 0.00 0.00 1.00 1.00 We can see from the picture above that the minimum value of Glucose, BloodPressure, SkinThickness, Insulin, BMI is zero. However, anyone having common sense knows that by no means would these indicators of an alive person be zero. Apparently, the null values are replaced by zeros when inputing data. Here we calculate the number of null values in each column so that we will better decide how to deal with them. for col in list(data.columns): zero = len(data[data[col] == 0]) print('0s in %s: %d' % (col, zero)) 0s in Pregnancies: 111 0s in Glucose: 5 0s in BloodPressure: 35 0s in SkinThickness: 227 0s in Insulin: 374 0s in BMI: 11 0s in DiabetesPedigreeFunction: 0 0s in Age: 0 0s in Outcome: 500 There are nearly 400 null values in Insulin column, so it's not realistic to drop all the null values. Normally under such circumstance, we replace them with indicators such as mean, median, or mode, etc. Here I choose median. # replace 0s in target columns with null value data.iloc[:, 1:6] = data.iloc[:, 1:6].replace({0:np.NaN}) # fill null values with medians data = data.fillna(data.median()) data.describe().round(2).T count mean std min 25% 50% 75% max Pregnancies 768.0 3.85 3.37 0.00 1.00 3.00 6.00 17.00 Glucose 768.0 121.66 30.44 44.00 99.75 117.00 140.25 199.00 BloodPressure 768.0 72.39 12.10 24.00 64.00 72.00 80.00 122.00 SkinThickness 768.0 29.11 8.79 7.00 25.00 29.00 32.00 99.00 Insulin 768.0 140.67 86.38 14.00 121.50 125.00 127.25 846.00 BMI 768.0 32.46 6.88 18.20 27.50 32.30 36.60 67.10 DiabetesPedigreeFunction 768.0 0.47 0.33 0.08 0.24 0.37 0.63 2.42 Age 768.0 33.24 11.76 21.00 24.00 29.00 41.00 81.00 Outcome 768.0 0.35 0.48 0.00 0.00 0.00 1.00 1.00 Now we can visualize the dataset and see if we can find something else. import seaborn as sns sns.pairplot(data, hue=&quot;Outcome&quot;, corner=True) From the pairplot above, we discover this is an imbalanced dataset. There are 500 observations whose outcome is 0 and only 268 is 1. If we train our model with such data, the results may be misleading. There are several methods to deal with imbalanced data: Collect more data Under-Sampling Over-Sampling Use confusion matrix or other way to evaluate the peformance As the number of observations is only 768, it's unrealistic to under-sampling. Here I wil illustrate how to do oversampling using SMOTE(). from imblearn.over_sampling import SMOTE # get X, y X = data.iloc[:, 0:8] y = data.iloc[:, -1] # perform SMOTE() oversample = SMOTE() X, y = oversample.fit_resample(X, y) # join X, y y = pd.DataFrame(y) data = pd.concat([X, y], axis=1) print(data.shape) data.describe().round(2).T count mean std min 25% 50% 75% max Pregnancies 1000.0 3.99 3.33 0.00 1.00 3.00 6.00 17.00 Glucose 1000.0 126.40 31.39 44.00 102.82 122.00 148.00 199.00 BloodPressure 1000.0 72.97 11.71 24.00 65.75 72.00 80.00 122.00 SkinThickness 1000.0 29.64 8.22 7.00 27.00 29.00 33.00 99.00 Insulin 1000.0 146.58 90.69 14.00 125.00 125.00 136.25 846.00 BMI 1000.0 33.01 6.68 18.20 28.40 32.66 36.80 67.10 DiabetesPedigreeFunction 1000.0 0.49 0.32 0.08 0.26 0.40 0.65 2.42 Age 1000.0 33.88 11.34 21.00 25.00 31.00 41.00 81.00 Outcome 1000.0 0.50 0.50 0.00 0.00 0.50 1.00 1.00 Once again visualize the dataset to see the changes in the distribution of data. sns.pairplot(data, hue=&quot;Outcome&quot;, corner=True) For now, we transform the original imbalanced dataset into a balanced dataset with the number of each outcome class being 500. there is not much difference in distribution between diabetics and nondiabetics. Glucose should have the highest correlation with Outcome among all the features. We can testify it using a correlation matrix heatmap. corr_mat = data.corr().round(2) plt.figure(figsize = (15,10)) sns.heatmap(corr_mat, annot=True) Data Transformation If the range of two features differ a lot, for example, one is from 0 to 1 and the other is from 0 to 10,000, apparantly models won't treat such two features equally. Normally, the feature with larger range would influence the result more. Nevertheless, this doesn't mean it's more important than the other one. That's why we need scale the features. Frequently used scaling techniques are: MinMaxScaler() StardardScaler() MaxAbsScaler() RobustScaler() I usually choose MinMaxScaler() or StandardScaler() for most of datasets. You can also use others according to the charasteristic of the dataset at hand. # get X, y X = data.drop([&quot;Outcome&quot;], axis=1) y = data[&quot;Outcome&quot;] X2 = X.copy y2 = y.copy from sklearn.model_selection import train_test_split # split X, y into traning set and validation set X2_train, X2_trainValid, y2_train, y2_trainValid = train_test_split(X, y, test_size=0.5, stratify=y) # perform feature scaling scaler = StandardScaler() X2_train = scaler.fit_transform(X2_train) print(X2_train) X2_trainValid = scaler.transform(X2_trainValid) print(X2_trainValid) [[-0.86583338 -1.18590252 -2.5407598 ... 2.36236055 0.059368 -0.66182567] [-0.56074762 -1.30437961 -0.5856297 ... 0.92385297 1.5766631 -0.575063 ] [-0.56074762 -0.84741167 -1.01001675 ... 0.49700038 -0.09947556 -1.09563904] ... [ 0.35450965 0.45821104 0.68753147 ... -0.21442061 -0.42275597 -0.74858835] [ 1.57485267 1.18460674 -0.30940607 ... -1.09502236 -0.52771647 1.07342779] [-0.56074762 -0.8147711 -0.33099746 ... 0.76734036 -0.55277092 -0.66182567]] [[ 1.57485267 -0.73832382 1.59549311 ... 1.66775686 1.24296528 0.55285175] [-0.86583338 -1.43494188 0.17826701 ... 1.8771571 2.15997341 -0.14124963] [-0.86583338 2.09023942 -1.01001675 ... -0.4278469 -0.29274103 2.20134255] ... [-0.86583338 -1.50022302 0.68753147 ... -1.09658263 0.35733372 -1.00887637] [-1.17091913 -0.03139747 -0.16124264 ... -0.81201424 -0.79874515 0.20580106] [-1.17091913 -1.66342586 -1.68903605 ... -0.75510056 -0.74603639 -1.00887637]] Data Reduction With clean data at hand, we can visulize the data, which may indicate some relations between the features or between features and targets. Knowing these is useful to data reduction, for example, if a feature is highly correlated with the target, then we had better retain it. Or if two features are highly correlated, it would be much easier for us if we drop out one of them. Here I use PCA (Principal componenet analysis) for a demonstration. from sklearn.decomposition import PCA pca = PCA() X = pca.fit_transform(X) print(X) [[-1.48212827e+01 2.66418913e+01 6.77045733e+00 ... 1.28306645e+00 -4.41730071e-01 1.08702215e-01] [-2.53614440e+01 -3.80054541e+01 -1.53466357e+00 ... 4.06783776e+00 -2.50459441e+00 -9.57021831e-02] [-9.83659118e+00 5.72071616e+01 -1.78166537e+01 ... 8.30157785e+00 4.60163646e+00 2.42241723e-01] ... [-8.54656132e+01 3.00921079e+01 6.86126007e+00 ... 4.57134748e+00 2.42255141e+00 1.22062151e-01] [-2.36693301e+01 -2.79343649e+01 -3.54949033e+00 ... -3.82012635e-01 4.69302230e-01 -2.89569829e-01] [-1.69091697e+01 1.34152393e+01 3.35830801e+00 ... -1.66109447e+01 5.12758994e+00 2.13862262e-02]] ","link":"https://qingqiuzhang.github.io/diabetes-dataset-report/"}]}