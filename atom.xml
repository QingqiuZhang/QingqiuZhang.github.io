<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://qingqiuzhang.github.io</id>
    <title>Qingqiu Zhang</title>
    <updated>2022-04-10T14:43:37.158Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://qingqiuzhang.github.io"/>
    <link rel="self" href="https://qingqiuzhang.github.io/atom.xml"/>
    <logo>https://qingqiuzhang.github.io/images/avatar.png</logo>
    <icon>https://qingqiuzhang.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, Qingqiu Zhang</rights>
    <entry>
        <title type="html"><![CDATA[Bike Sharing Dataset Analytics (2) -- Hourly Data]]></title>
        <id>https://qingqiuzhang.github.io/bike-sharing-dadaset-analytics/</id>
        <link href="https://qingqiuzhang.github.io/bike-sharing-dadaset-analytics/">
        </link>
        <updated>2022-04-06T21:17:04.000Z</updated>
        <content type="html"><![CDATA[<p>In the <a href="https://qingqiuzhang.github.io/bike-sharing-dataset-analytics/">previous article</a>, I analyze the bike sharing dataset from a daily level. In this article, I will do a similar procedure from a houly level.<br>
Source data: <a href="https://github.com/qingqiuzhang/qingqiuzhang.github.io/blob/data/bike-sharing-dataset-analytics-hourly-data/hour.csv">hour.csv</a><br>
Source python file: <a href="https://github.com/qingqiuzhang/qingqiuzhang.github.io/blob/data/bike-sharing-dataset-analytics-hourly-data/Bike%20Sharing%20Dataset%20Analytics%20--%20Hourly%20Data.py">Bike Sharing Dataset Analytics -- Hourly Data.py</a></p>
<h1 id="exploring-the-data">Exploring the Data</h1>
<pre><code class="language-Python">import pandas as pd
hour = pd.read_csv(&quot;hour.csv&quot;)
hour.head()
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>instant</th>
      <th>dteday</th>
      <th>season</th>
      <th>yr</th>
      <th>mnth</th>
      <th>hr</th>
      <th>holiday</th>
      <th>weekday</th>
      <th>workingday</th>
      <th>weathersit</th>
      <th>temp</th>
      <th>atemp</th>
      <th>hum</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>1</td>
      <td>0.24</td>
      <td>0.2879</td>
      <td>0.81</td>
      <td>0.0</td>
      <td>3</td>
      <td>13</td>
      <td>16</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>1</td>
      <td>0.22</td>
      <td>0.2727</td>
      <td>0.80</td>
      <td>0.0</td>
      <td>8</td>
      <td>32</td>
      <td>40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>1</td>
      <td>0.22</td>
      <td>0.2727</td>
      <td>0.80</td>
      <td>0.0</td>
      <td>5</td>
      <td>27</td>
      <td>32</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>1</td>
      <td>0.24</td>
      <td>0.2879</td>
      <td>0.75</td>
      <td>0.0</td>
      <td>3</td>
      <td>10</td>
      <td>13</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>1</td>
      <td>0.24</td>
      <td>0.2879</td>
      <td>0.75</td>
      <td>0.0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<h1 id="visualization">Visualization</h1>
<pre><code class="language-Python">import matplotlib.pyplot as plt
result = hour[[&quot;hr&quot;, &quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].groupby([&quot;hr&quot;]).mean()
result = (
    result.stack()
    .reset_index()
    .set_index(&quot;hr&quot;)
    .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people per hour&quot;})
)
f, axes = plt.subplots(2, sharey=False, figsize=(25, 12))
sns.barplot(x=result.index, y=&quot;people per hour&quot;, hue=&quot;cat&quot;, data=result, ax=axes[0])

result = hour[[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].set_index(hour[&quot;hr&quot;])
result = (
    result.stack()
    .reset_index()
    .set_index(&quot;hr&quot;)
    .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people&quot;})
)
sns.violinplot(x=result.index, y=&quot;people&quot;, hue=&quot;cat&quot;, data=result, cut=0, ax=axes[1])
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://qingqiuzhang.github.io/post-images/1649544710526.png" alt="" loading="lazy"></figure>
<h1 id="linear-regression">Linear Regression</h1>
<h2 id="split-data">Split Data</h2>
<pre><code class="language-Python">from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

cat       = [&quot;season&quot;, &quot;mnth&quot;, &quot;hr&quot;, &quot;holiday&quot;, &quot;weekday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;]
hour[cat] = hour[cat].apply(lambda x: x.astype(&quot;category&quot;))
dummies   = pd.get_dummies(hour[cat], drop_first=True)
print(dummies.shape)
conti_predictors = [&quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;]
print(hour[conti_predictors].shape)
X = pd.concat([hour[conti_predictors], dummies], axis=1)
X.head()
y = hour.iloc[:, 14:]
y.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
sc = StandardScaler()
X_train[conti_predictors] = sc.fit_transform(X_train[conti_predictors])
X_test[conti_predictors] = sc.transform(X_test[conti_predictors])
</code></pre>
<pre>(17379, 48)
(17379, 4)</pre>
<div style="overflow: auto; width: 780px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>temp</th>
      <th>atemp</th>
      <th>hum</th>
      <th>windspeed</th>
      <th>season_2</th>
      <th>season_3</th>
      <th>season_4</th>
      <th>mnth_2</th>
      <th>mnth_3</th>
      <th>mnth_4</th>
      <th>mnth_5</th>
      <th>mnth_6</th>
      <th>mnth_7</th>
      <th>mnth_8</th>
      <th>mnth_9</th>
      <th>mnth_10</th>
      <th>mnth_11</th>
      <th>mnth_12</th>
      <th>hr_1</th>
      <th>hr_2</th>
      <th>hr_3</th>
      <th>hr_4</th>
      <th>hr_5</th>
      <th>hr_6</th>
      <th>hr_7</th>
      <th>hr_8</th>
      <th>hr_9</th>
      <th>hr_10</th>
      <th>hr_11</th>
      <th>hr_12</th>
      <th>hr_13</th>
      <th>hr_14</th>
      <th>hr_15</th>
      <th>hr_16</th>
      <th>hr_17</th>
      <th>hr_18</th>
      <th>hr_19</th>
      <th>hr_20</th>
      <th>hr_21</th>
      <th>hr_22</th>
      <th>hr_23</th>
      <th>holiday_1</th>
      <th>weekday_1</th>
      <th>weekday_2</th>
      <th>weekday_3</th>
      <th>weekday_4</th>
      <th>weekday_5</th>
      <th>weekday_6</th>
      <th>workingday_1</th>
      <th>weathersit_2</th>
      <th>weathersit_3</th>
      <th>weathersit_4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.24</td>
      <td>0.2879</td>
      <td>0.81</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.22</td>
      <td>0.2727</td>
      <td>0.80</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.22</td>
      <td>0.2727</td>
      <td>0.80</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.24</td>
      <td>0.2879</td>
      <td>0.75</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.24</td>
      <td>0.2879</td>
      <td>0.75</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<div style="overflow: auto; width: 780px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>13</td>
      <td>16</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8</td>
      <td>32</td>
      <td>40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5</td>
      <td>27</td>
      <td>32</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>10</td>
      <td>13</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<h2 id="train-model">Train Model</h2>
<pre><code class="language-Python3">from sklearn.linear_model import LinearRegression
from dmba import adjusted_r2_score

lr = LinearRegression()
lr.fit(X_train, y_train)

y_pred = lr.predict(X_test)
y_pred = pd.DataFrame(y_pred, columns=[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]).astype(int)
print(y_pred)
print(adjusted_r2_score(y_test, y_pred, lr))
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>37</td>
      <td>249</td>
      <td>286</td>
    </tr>
    <tr>
      <th>1</th>
      <td>42</td>
      <td>344</td>
      <td>386</td>
    </tr>
    <tr>
      <th>2</th>
      <td>48</td>
      <td>259</td>
      <td>307</td>
    </tr>
    <tr>
      <th>3</th>
      <td>19</td>
      <td>360</td>
      <td>380</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-9</td>
      <td>-88</td>
      <td>-98</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4340</th>
      <td>6</td>
      <td>205</td>
      <td>211</td>
    </tr>
    <tr>
      <th>4341</th>
      <td>6</td>
      <td>109</td>
      <td>115</td>
    </tr>
    <tr>
      <th>4342</th>
      <td>32</td>
      <td>226</td>
      <td>259</td>
    </tr>
    <tr>
      <th>4343</th>
      <td>52</td>
      <td>93</td>
      <td>146</td>
    </tr>
    <tr>
      <th>4344</th>
      <td>-5</td>
      <td>142</td>
      <td>137</td>
    </tr>
  </tbody>
</table>
</div>
<pre>0.5979873068757086</pre>
Because number of people cannot be negative, we change some data so that the result make sense.
<pre><code class="language-Python">for i in range(y_pred.shape[0]):
    if y_pred[&quot;casual&quot;].iloc[i] &lt; 0:
        y_pred[&quot;casual&quot;].iloc[i] = 0
    if y_pred[&quot;registered&quot;].iloc[i] &lt; 0:
        y_pred[&quot;registered&quot;].iloc[i] = 0
        y_pred[&quot;cnt&quot;].iloc[i] = y_pred[&quot;casual&quot;].iloc[i] + y_pred[&quot;registered&quot;].iloc[i]
print(y_pred)
print(adjusted_r2_score(y_test, y_pred, lr))
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>37</td>
      <td>249</td>
      <td>286</td>
    </tr>
    <tr>
      <th>1</th>
      <td>42</td>
      <td>344</td>
      <td>386</td>
    </tr>
    <tr>
      <th>2</th>
      <td>48</td>
      <td>259</td>
      <td>307</td>
    </tr>
    <tr>
      <th>3</th>
      <td>19</td>
      <td>360</td>
      <td>380</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>4340</th>
      <td>6</td>
      <td>205</td>
      <td>211</td>
    </tr>
    <tr>
      <th>4341</th>
      <td>6</td>
      <td>109</td>
      <td>115</td>
    </tr>
    <tr>
      <th>4342</th>
      <td>32</td>
      <td>226</td>
      <td>259</td>
    </tr>
    <tr>
      <th>4343</th>
      <td>52</td>
      <td>93</td>
      <td>146</td>
    </tr>
    <tr>
      <th>4344</th>
      <td>0</td>
      <td>142</td>
      <td>142</td>
    </tr>
  </tbody>
</table>
</div>
<pre>0.6153434162490663</pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Iris Dataset Analytics]]></title>
        <id>https://qingqiuzhang.github.io/iris-data-set-analytics/</id>
        <link href="https://qingqiuzhang.github.io/iris-data-set-analytics/">
        </link>
        <updated>2022-03-25T15:15:02.000Z</updated>
        <content type="html"><![CDATA[<p>The Iris Data set was created by R.A. Fisher and is perhaps the best known data set to be found in the pattern recognition literature. Fisherâ€™s paper is a classic in the field and is referenced frequently to this day. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. Predicted attribute: class of iris plant.</p>
<p>This article aims to classify this dataset utilizing the following models: K Nearest Neighbors (KNN), Naive Bayes and Logistic Regression.<br>
You can find the <a href="https://github.com/qingqiuzhang/qingqiuzhang.github.io/blob/data/iris-data-set-analytics/iris_dataset.csv">data source</a> and <a href="https://github.com/qingqiuzhang/qingqiuzhang.github.io/blob/data/iris-data-set-analytics/Iris%20Dataset%20Analytics.py">Iris Dataset Analytics.py</a> here.</p>
<h1 id="exploring-data">Exploring Data</h1>
<pre><code class="language-Python">import pandas as pd
import numpy as np

df = pd.read_csv(
    &quot;/Users/qingqiuzhang/Desktop/iris_dataset.csv&quot;,
    header = None,
    names  = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;, &quot;class&quot;],
)
df.head()
</code></pre>
<table border="1" align="center">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length</th>
      <th>sepal width</th>
      <th>petal length</th>
      <th>petal width</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
  </tbody>
</table>
<p>Calculate some indicators of features.</p>
<pre><code class="language-Python">df.describe()
</code></pre>
<table border="1" align="center">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length</th>
      <th>sepal width</th>
      <th>petal length</th>
      <th>petal width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>150.000000</td>
      <td>150.000000</td>
      <td>150.000000</td>
      <td>150.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>5.843333</td>
      <td>3.057333</td>
      <td>3.758000</td>
      <td>1.199333</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.828066</td>
      <td>0.435866</td>
      <td>1.765298</td>
      <td>0.762238</td>
    </tr>
    <tr>
      <th>min</th>
      <td>4.300000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>0.100000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>5.100000</td>
      <td>2.800000</td>
      <td>1.600000</td>
      <td>0.300000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>5.800000</td>
      <td>3.000000</td>
      <td>4.350000</td>
      <td>1.300000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>6.400000</td>
      <td>3.300000</td>
      <td>5.100000</td>
      <td>1.800000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>7.900000</td>
      <td>4.400000</td>
      <td>6.900000</td>
      <td>2.500000</td>
    </tr>
  </tbody>
</table>
## Visualization
<pre><code class="language-Python">import matplotlib.pyplot as plt
import seaborn as sns

features = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;]
sns.set(style=&quot;ticks&quot;, palette=&quot;pastel&quot;)
f, axes = plt.subplots(2, 2, sharey=False, figsize=(14, 14))
for ind, val in enumerate(features):
    sns.violinplot(x=&quot;class&quot;, y=val, data=df, ax=axes[ind // 2, ind % 2]).set(
        title = &quot;Sepal Length&quot;
    )

plt.show()
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://qingqiuzhang.github.io/post-images/1648956002552.png" alt="" loading="lazy"></figure>
<pre><code class="language-Python">sns.pairplot(df, hue=&quot;class&quot;)
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://qingqiuzhang.github.io/post-images/1648956073180.png" alt="" loading="lazy"></figure>
<h1 id="train-and-evaluate-three-models-using-cross-validation">Train and evaluate three models using cross-validation</h1>
<h2 id="k-nearest-neighbors-classifier">K-Nearest Neighbors Classifier</h2>
<pre><code class="language-Python">from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import ShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

NUM         = 200
X           = df.drop([&quot;class&quot;], axis=1)
y           = df[&quot;class&quot;]
shuffle     = ShuffleSplit(n_splits=NUM, test_size=0.25, random_state=10)
results     = []
klist       = np.arange(1, 21, 1)
FullModel   = Pipeline([(&quot;scaler&quot;, StandardScaler()), (&quot;knn&quot;, KNeighborsClassifier())])
param_grid  = {&quot;knn__n_neighbors&quot;: klist}
grid_search = GridSearchCV(
    FullModel,
    param_grid,
    scoring            = &quot;accuracy&quot;,
    cv                 = shuffle,
    return_train_score = True,
    n_jobs             = -1,
)
grid_search.fit(X, y)
results = pd.DataFrame(grid_search.cv_results_)

print(
    results[
        [
            &quot;rank_test_score&quot;,
            &quot;mean_train_score&quot;,
            &quot;mean_test_score&quot;,
            &quot;param_knn__n_neighbors&quot;,
        ]
    ]
)
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank_test_score</th>
      <th>mean_train_score</th>
      <th>mean_test_score</th>
      <th>param_knn__n_neighbors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18</td>
      <td>1.000000</td>
      <td>0.943947</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20</td>
      <td>0.970268</td>
      <td>0.937368</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17</td>
      <td>0.959018</td>
      <td>0.945132</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>15</td>
      <td>0.958214</td>
      <td>0.945789</td>
      <td>4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12</td>
      <td>0.963304</td>
      <td>0.949737</td>
      <td>5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>9</td>
      <td>0.963080</td>
      <td>0.952105</td>
      <td>6</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>0.966741</td>
      <td>0.954605</td>
      <td>7</td>
    </tr>
    <tr>
      <th>7</th>
      <td>3</td>
      <td>0.962500</td>
      <td>0.955395</td>
      <td>8</td>
    </tr>
    <tr>
      <th>8</th>
      <td>7</td>
      <td>0.963482</td>
      <td>0.954474</td>
      <td>9</td>
    </tr>
    <tr>
      <th>9</th>
      <td>5</td>
      <td>0.963571</td>
      <td>0.955000</td>
      <td>10</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2</td>
      <td>0.965536</td>
      <td>0.956447</td>
      <td>11</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1</td>
      <td>0.965982</td>
      <td>0.958553</td>
      <td>12</td>
    </tr>
    <tr>
      <th>12</th>
      <td>4</td>
      <td>0.963973</td>
      <td>0.955132</td>
      <td>13</td>
    </tr>
    <tr>
      <th>13</th>
      <td>8</td>
      <td>0.963661</td>
      <td>0.954079</td>
      <td>14</td>
    </tr>
    <tr>
      <th>14</th>
      <td>11</td>
      <td>0.961205</td>
      <td>0.950000</td>
      <td>15</td>
    </tr>
    <tr>
      <th>15</th>
      <td>10</td>
      <td>0.959107</td>
      <td>0.950658</td>
      <td>16</td>
    </tr>
    <tr>
      <th>16</th>
      <td>14</td>
      <td>0.957143</td>
      <td>0.946711</td>
      <td>17</td>
    </tr>
    <tr>
      <th>17</th>
      <td>13</td>
      <td>0.956205</td>
      <td>0.947895</td>
      <td>18</td>
    </tr>
    <tr>
      <th>18</th>
      <td>16</td>
      <td>0.955223</td>
      <td>0.945526</td>
      <td>19</td>
    </tr>
    <tr>
      <th>19</th>
      <td>19</td>
      <td>0.953036</td>
      <td>0.942895</td>
      <td>20</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code class="language-Python"># Plot Accuracy vs K
fig, ax = plt.subplots()
ax.plot(
    results[&quot;param_knn__n_neighbors&quot;], results[&quot;mean_test_score&quot;], label=&quot;test accuracy&quot;
)
ax.set_xlim(15, 0)  # reverse x; from simple model to complex model
# (complex model tries hard to sort of figure out all sorts of details in the data)
ax.set_ylabel(&quot;Accuracy&quot;)
ax.set_xlabel(&quot;n_neighbors&quot;)
ax.grid()
ax.legend()
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://qingqiuzhang.github.io/post-images/1649519204065.png" alt="" loading="lazy"></figure>
<h2 id="naive-bayes-classifier">Naive Bayes Classifier</h2>
<pre><code class="language-Python">from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import MinMaxScaler
features = [&quot;sepal length&quot;, &quot;sepal width&quot;, &quot;petal length&quot;, &quot;petal width&quot;]

df1 = df.copy()
for column in features:
    df1[column] = [round(i - np.min(df1[column])) for i in df1[column]]
    df1[column] = df1[column].astype(&quot;category&quot;)
    X = pd.get_dummies(df1[features])

# Calculate mean test accuracy
alphas      = [0.01, 0.1, 1.0, 5.0, 10.0, 15.0, 20.0, 35.0, 50.0]
FullModel   = Pipeline([(&quot;scaler&quot;, MinMaxScaler()), (&quot;mnb&quot;, MultinomialNB())])
param_grid  = {&quot;mnb__alpha&quot;: alphas}
grid_search = GridSearchCV(
    FullModel,
    param_grid,
    scoring            = &quot;accuracy&quot;,
    cv                 = shuffle,
    return_train_score = True,
    n_jobs             = -1,
)
grid_search.fit(X, y)
results = pd.DataFrame(grid_search.cv_results_)
print(
    results[
        [&quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_mnb__alpha&quot;]
    ]
)
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank_test_score</th>
      <th>mean_train_score</th>
      <th>mean_test_score</th>
      <th>param_mnb__alpha</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9</td>
      <td>0.942054</td>
      <td>0.931711</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8</td>
      <td>0.942187</td>
      <td>0.932105</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7</td>
      <td>0.944420</td>
      <td>0.935789</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5</td>
      <td>0.947321</td>
      <td>0.942237</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>0.946830</td>
      <td>0.945132</td>
      <td>10.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1</td>
      <td>0.946786</td>
      <td>0.946184</td>
      <td>15.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>2</td>
      <td>0.946741</td>
      <td>0.946053</td>
      <td>20.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>4</td>
      <td>0.945312</td>
      <td>0.944211</td>
      <td>35.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>6</td>
      <td>0.943080</td>
      <td>0.941053</td>
      <td>50.0</td>
    </tr>
  </tbody>
</table>
</div>
<h2 id="logistic-regression">Logistic Regression</h2>
<pre><code class="language-Python">from sklearn.linear_model import LogisticRegression

df2 = df.copy()
X   = df2.iloc[:, 0:4].values
y   = df2.iloc[:, 4].values

# Calculate mean test accuracy
Clist     = [0.01, 0.1, 1.0, 5.0, 10.0, 20.0, 50.0, 100.0, 150.0, 200.0]
FullModel = Pipeline(
    [
        (&quot;scaler&quot;, StandardScaler()),
        (&quot;lr&quot;, LogisticRegression(penalty=&quot;l2&quot;, solver=&quot;lbfgs&quot;, multi_class=&quot;auto&quot;)),
    ]
)
param_grid  = {&quot;lr__C&quot;: Clist}
grid_search = GridSearchCV(
    FullModel,
    param_grid,
    scoring            = &quot;accuracy&quot;,
    cv                 = shuffle,
    return_train_score = True,
    n_jobs             = -1,
)
grid_search.fit(X, y)
results = pd.DataFrame(grid_search.cv_results_)
print(
    results[[&quot;rank_test_score&quot;, &quot;mean_train_score&quot;, &quot;mean_test_score&quot;, &quot;param_lr__C&quot;]]
)
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>rank_test_score</th>
      <th>mean_train_score</th>
      <th>mean_test_score</th>
      <th>param_lr__C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>10</td>
      <td>0.852321</td>
      <td>0.834342</td>
      <td>0.01</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9</td>
      <td>0.919777</td>
      <td>0.908684</td>
      <td>0.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>0.969509</td>
      <td>0.960132</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7</td>
      <td>0.979420</td>
      <td>0.968026</td>
      <td>5.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>0.981295</td>
      <td>0.969342</td>
      <td>10.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3</td>
      <td>0.982857</td>
      <td>0.969737</td>
      <td>20.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1</td>
      <td>0.984598</td>
      <td>0.970263</td>
      <td>50.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2</td>
      <td>0.986205</td>
      <td>0.969737</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>5</td>
      <td>0.986786</td>
      <td>0.969211</td>
      <td>150.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>6</td>
      <td>0.987009</td>
      <td>0.968684</td>
      <td>200.0</td>
    </tr>
  </tbody>
</table>
</div>
<h1 id="conclusion">Conclusion</h1>
<p>For the current running, Logistic Regression has the highest test accuracy followed by KNN Classifier. Naive Bayes Classifier has the lowest test accuracy.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bike Sharing Dataset Analytics (1) -- Daily Data]]></title>
        <id>https://qingqiuzhang.github.io/bike-sharing-dataset-analytics/</id>
        <link href="https://qingqiuzhang.github.io/bike-sharing-dataset-analytics/">
        </link>
        <updated>2022-03-20T03:31:35.000Z</updated>
        <content type="html"><![CDATA[<p>This article aims to predict the count of casual users (feature casual), count of registered users (feature registered), and the total count of both causal and registered users (feature cnt) using the multi-regression model.<br>
You can go to this webpage to find the source data <a href="https://github.com/qingqiuzhang/qingqiuzhang.github.io/blob/data/bike-sharing-dataset-analytics-daily-data/day.csv">day.csv</a> and also you can find <a href="https://github.com/qingqiuzhang/qingqiuzhang.github.io/blob/data/bike-sharing-dataset-analytics-daily-data/Bike%20Sharing%20Dataset%20Analytics%20--%20Daily%20Data.py">Bike Sharing Dataset Analytics -- Daily Data.py</a> here.</p>
<h1 id="exploring-the-data">Exploring the data</h1>
<pre><code class="language-Python">import pandas as pd
day = pd.read_csv(&quot;day.csv&quot;)
day.head()
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="0">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>instant</th>
      <th>dteday</th>
      <th>season</th>
      <th>yr</th>
      <th>mnth</th>
      <th>holiday</th>
      <th>weekday</th>
      <th>workingday</th>
      <th>weathersit</th>
      <th>temp</th>
      <th>atemp</th>
      <th>hum</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr> 
      <th>0</th>
      <td>1</td>
      <td nowrap="nowrap">2011-01-01</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>2</td>
      <td>0.344167</td>
      <td>0.363625</td>
      <td>0.805833</td>
      <td>0.160446</td>
      <td>331</td>
      <td>654</td>
      <td>985</td>
    </tr>
    <tr> 
      <th>1</th>
      <td>2</td>
      <td nowrap="nowrap">2011-01-02</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0.363478</td>
      <td>0.353739</td>
      <td>0.696087</td>
      <td>0.248539</td>
      <td>131</td>
      <td>670</td>
      <td>801</td>
    </tr>
    <tr> 
      <th>2</th>
      <td>3</td>
      <td nowrap="nowrap">2011-01-03</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0.196364</td>
      <td>0.189405</td>
      <td>0.437273</td>
      <td>0.248309</td>
      <td>120</td>
      <td>1229</td>
      <td>1349</td>
    </tr>
    <tr> 
      <th>3</th>
      <td>4</td>
      <td nowrap="nowrap">2011-01-04</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0.200000</td>
      <td>0.212122</td>
      <td>0.590435</td>
      <td>0.160296</td>
      <td>108</td>
      <td>1454</td>
      <td>1562</td>
    </tr>
    <tr> 
      <th>4</th>
      <td>5</td>
      <td nowrap="nowrap">2011-01-05</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>0.226957</td>
      <td>0.229270</td>
      <td>0.436957</td>
      <td>0.186900</td>
      <td>82</td>
      <td>1518</td>
      <td>1600</td>
    </tr>
  </tbody>
</table>
</div>
<h2 id="visualization">Visualization</h2>
<pre><code class="language-Python">import matplotlib.pyplot as plt
import seaborn as sns

result = day[[&quot;mnth&quot;, &quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].groupby([&quot;mnth&quot;]).mean()
result = (
    result.stack()
    .reset_index()
    .set_index(&quot;mnth&quot;)
    .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people per day&quot;})
)
cat    = [&quot;season&quot;, &quot;yr&quot;, &quot;holiday&quot;, &quot;weekday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;]

sns.set(style=&quot;ticks&quot;, palette=&quot;pastel&quot;)
f, axes = plt.subplots(3, 3, sharey=False, figsize=(15, 12))
ax      = plt.subplot2grid((3, 3), (0, 0), colspan=3)
sns.barplot(x=result.index, y=&quot;people per day&quot;, data=result, hue=&quot;cat&quot;, ax=ax)
for ind, val in enumerate(cat):
    result = round(day[[val, &quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].groupby([val]).mean())
    result = (
        result.stack()
        .reset_index()
        .set_index(val)
        .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people per day&quot;})
    )
    sns.barplot(
        x    = result.index,
        y    = &quot;people per day&quot;,
        data = result,
        hue  = &quot;cat&quot;,
        ax   = axes[ind // 3 + 1, ind % 3],
    )
f.tight_layout(pad=3.0)
plt.show()
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://qingqiuzhang.github.io/post-images/1648957479910.png" alt="" loading="lazy"></figure>
<pre><code class="language-Python">result = day[[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].set_index(day[&quot;mnth&quot;])
result = (
    result.stack()
    .reset_index()
    .set_index(&quot;mnth&quot;)
    .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people&quot;})
)

f, axes = plt.subplots(3, 3, sharey=False, figsize=(20, 12))
ax      = plt.subplot2grid((3, 3), (0, 0), colspan=3)
sns.violinplot(x=result.index, y=&quot;people&quot;, hue=&quot;cat&quot;, data=result, cut=0, ax=ax)
for ind, val in enumerate(cat):
    result = day[[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]].set_index(day[val])
    result = (
        result.stack()
        .reset_index()
        .set_index(val)
        .rename(columns={&quot;level_1&quot;: &quot;cat&quot;, 0: &quot;people&quot;})
    )
    sns.violinplot(
        x    = result.index,
        y    = &quot;people&quot;,
        hue  = &quot;cat&quot;,
        data = result,
        cut  = 0,
        ax   = axes[ind // 3 + 1, ind % 3],
    )
f.tight_layout(pad=3.0)
plt.show()
</code></pre>
<p><img src="https://qingqiuzhang.github.io/post-images/1648959164123.png" alt="" loading="lazy"><br>
The number of people who rental a bike every year is increasing, which indicates that riding bicycle is getting popular. This trend could be got if we fit a model on date and year. But here, since we don't have data of enough years, so we maily focus on the other features in our analysis.</p>
<h1 id="linear-regression">Linear Regression</h1>
<h2 id="split-data">Split Data</h2>
<pre><code class="language-Python">from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

day[cat] = day[cat].apply(lambda x: x.astype(&quot;category&quot;))
dummies  = pd.get_dummies(day[cat], drop_first=True)
print(dummies.shape)

conti_predictors = [&quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;]
print(day[conti_predictors].shape)
X = pd.concat([day[conti_predictors], dummies], axis=1)
y = day.iloc[:, 13:]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
sc                               = StandardScaler()
X_train[conti_predictors]        = sc.fit_transform(X_train[conti_predictors])
X_test [conti_predictors]        = sc.transform(X_test[conti_predictors])
</code></pre>
<pre>(731, 24)
(731, 4)</pre>
<h2 id="train-model">Train Model</h2>
<pre><code class="language-Python">from sklearn.linear_model import LinearRegression
from dmba import adjusted_r2_score

lr     = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
y_pred = pd.DataFrame(y_pred, columns=[&quot;casual&quot;, &quot;registered&quot;, &quot;cnt&quot;]).astype(int)
print(y_pred)
print(adjusted_r2_score(y_test, y_pred, lr))
</code></pre>
<table border="1" align="center">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1119</td>
      <td>5058</td>
      <td>6178</td>
    </tr>
    <tr>
      <th>1</th>
      <td>886</td>
      <td>3975</td>
      <td>4861</td>
    </tr>
    <tr>
      <th>2</th>
      <td>961</td>
      <td>5148</td>
      <td>6109</td>
    </tr>
    <tr>
      <th>3</th>
      <td>865</td>
      <td>5510</td>
      <td>6375</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-94</td>
      <td>2389</td>
      <td>2294</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>178</th>
      <td>317</td>
      <td>2450</td>
      <td>2768</td>
    </tr>
    <tr>
      <th>179</th>
      <td>-5</td>
      <td>2653</td>
      <td>2647</td>
    </tr>
    <tr>
      <th>180</th>
      <td>1252</td>
      <td>4690</td>
      <td>5942</td>
    </tr>
    <tr>
      <th>181</th>
      <td>465</td>
      <td>2813</td>
      <td>3278</td>
    </tr>
    <tr>
      <th>182</th>
      <td>482</td>
      <td>3775</td>
      <td>4258</td>
    </tr>
  </tbody>
</table>
<pre>0.5664326431246609</pre>
Because number of people cannot be negative, we change some data so that the result make sense.
<pre><code class="language-Python">for i in range(y_pred.shape[0]):
    if y_pred[&quot;casual&quot;].iloc[i] &lt; 0:
        y_pred[&quot;casual&quot;].iloc[i] = 0
    if y_pred[&quot;registered&quot;].iloc[i] &lt; 0:
        y_pred[&quot;registered&quot;].iloc[i] = 0
        y_pred[&quot;cnt&quot;].iloc[i]        = y_pred[&quot;casual&quot;].iloc[i] + y_pred[&quot;registered&quot;].iloc[i]
print(y_pred)
print(adjusted_r2_score(y_test, y_pred, lr))
</code></pre>
<table border="1" align="center">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>casual</th>
      <th>registered</th>
      <th>cnt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1119</td>
      <td>5058</td>
      <td>6178</td>
    </tr>
    <tr>
      <th>1</th>
      <td>886</td>
      <td>3975</td>
      <td>4861</td>
    </tr>
    <tr>
      <th>2</th>
      <td>961</td>
      <td>5148</td>
      <td>6109</td>
    </tr>
    <tr>
      <th>3</th>
      <td>865</td>
      <td>5510</td>
      <td>6375</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>2389</td>
      <td>2389</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>178</th>
      <td>317</td>
      <td>2450</td>
      <td>2768</td>
    </tr>
    <tr>
      <th>179</th>
      <td>0</td>
      <td>2653</td>
      <td>2653</td>
    </tr>
    <tr>
      <th>180</th>
      <td>1252</td>
      <td>4690</td>
      <td>5942</td>
    </tr>
    <tr>
      <th>181</th>
      <td>465</td>
      <td>2813</td>
      <td>3278</td>
    </tr>
    <tr>
      <th>182</th>
      <td>482</td>
      <td>3775</td>
      <td>4258</td>
    </tr>
  </tbody>
</table>
<pre>0.5761640858277055</pre>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[National Center for Education Dataset Project]]></title>
        <id>https://qingqiuzhang.github.io/national-center-for-education-dataset-project/</id>
        <link href="https://qingqiuzhang.github.io/national-center-for-education-dataset-project/">
        </link>
        <updated>2022-03-17T02:19:52.000Z</updated>
        <content type="html"><![CDATA[<p>The dataset you will be processing comes from the National Center for Education Statistics. You can find the original dataset <a href="https://nces.ed.gov/programs/digest/d18/tables/dt18_104.20.asp">here</a>. This data has been cleaned a bit and you can use the the provided file <a href="https://github.com/qingqiuzhang/qingqiuzhang.github.io/blob/data/national-center-for-education-dataset-project/nces-ed-attainment.csv">nces-ed-attainment.csv</a>. You can find the whole python file <a href="https://github.com/qingqiuzhang/qingqiuzhang.github.io/blob/data/national-center-for-education-dataset-project/National%20Center%20for%20Education.py">National Center for Education.py</a> here.</p>
<p>The original dataset is titled: Percentage of persons 25 to 29 years old with selected levels of educational attainment, by race/ethnicity and sex: Selected years, 1920 through 2018. The cleaned version you will be working with has columns for Year, Sex, Educational Attainment, and race/ethnicity categories considered in the dataset. Note that not all columns will have data starting at 1920.</p>
<h1 id="exploring-the-data">Exploring the data</h1>
<pre><code class="language-Python"># packages needed
import pandas as pd
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('nces-ed-attainment.csv')
df.head()
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Year</th>
      <th>Sex</th>
      <th>Min degree</th>
      <th>Total</th>
      <th>White</th>
      <th>Black</th>
      <th>Hispanic</th>
      <th>Asian</th>
      <th>Pacific Islander</th>
      <th>American Indian/Alaska Native</th>
      <th>Two or more races</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1920</td>
      <td>A</td>
      <td>high school</td>
      <td>---</td>
      <td>22.0</td>
      <td>6.3</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
    </tr>
    <tr">
      <th>1</th>
      <td>1940</td>
      <td>A</td>
      <td>high school</td>
      <td>38.1</td>
      <td>41.2</td>
      <td>12.3</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1950</td>
      <td>A</td>
      <td>high school</td>
      <td>52.8</td>
      <td>56.3</td>
      <td>23.6</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1960</td>
      <td>A</td>
      <td>high school</td>
      <td>60.7</td>
      <td>63.7</td>
      <td>38.6</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1970</td>
      <td>A</td>
      <td>high school</td>
      <td>75.4</td>
      <td>77.8</td>
      <td>58.4</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
      <td>---</td>
    </tr>
  </tbody>
</table>
</div>
There are so many "---" in dataset. We should convert it into null values for further manipulation.
<pre><code class="language-Python">df = df.replace('---', np.nan)
df.info()
</code></pre>
<pre>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 214 entries, 0 to 213
Data columns (total 11 columns):
 #   Column                         Non-Null Count  Dtype  
---  ------                         --------------  -----  
 0   Year                           214 non-null    int64  
 1   Sex                            214 non-null    object 
 2   Min degree                     214 non-null    object 
 3   Total                          214 non-null    object 
 4   White                          214 non-null    float64
 5   Black                          214 non-null    float64
 6   Hispanic                       214 non-null    object 
 7   Asian                          214 non-null    object 
 8   Pacific Islander               214 non-null    object 
 9   American Indian/Alaska Native  214 non-null    object 
 10  Two or more races              214 non-null    object 
dtypes: float64(2), int64(1), object(8)
memory usage: 18.5+ KB
</pre>
<p>Several columns have wrong type.</p>
<pre><code class="language-Python3">race = [
    &quot;Total&quot;,
    &quot;Hispanic&quot;,
    &quot;Asian&quot;,
    &quot;Pacific Islander&quot;,
    &quot;American Indian/Alaska Native&quot;,
    &quot;Two or more races&quot;,
]
df[race] = df[race].apply(pd.to_numeric)
df.info()
</code></pre>
<pre>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 214 entries, 0 to 213
Data columns (total 11 columns):
 #   Column                         Non-Null Count  Dtype  
---  ------                         --------------  -----  
 0   Year                           214 non-null    int64  
 1   Sex                            214 non-null    object 
 2   Min degree                     214 non-null    object 
 3   Total                          212 non-null    float64
 4   White                          214 non-null    float64
 5   Black                          214 non-null    float64
 6   Hispanic                       204 non-null    float64
 7   Asian                          168 non-null    float64
 8   Pacific Islander               93 non-null     float64
 9   American Indian/Alaska Native  131 non-null    float64
 10  Two or more races              157 non-null    float64
dtypes: float64(8), int64(1), object(2)
memory usage: 18.5+ KB
</pre>
<p>Have a look at some indicators of each column.</p>
<pre><code class="language-Python">df.describe(include=&quot;all&quot;).T
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>unique</th>
      <th>top</th>
      <th>freq</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Year</th>
      <td>214.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2005.48</td>
      <td>15.6</td>
      <td>1920.0</td>
      <td>2005.0</td>
      <td>2010.0</td>
      <td>2014.0</td>
      <td>2018.0</td>
    </tr>
    <tr>
      <th>Sex</th>
      <td>214</td>
      <td>3</td>
      <td>A</td>
      <td>78</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Min degree</th>
      <td>214</td>
      <td>4</td>
      <td>high school</td>
      <td>59</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>Total</th>
      <td>212.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>42.77</td>
      <td>30.15</td>
      <td>4.1</td>
      <td>22.12</td>
      <td>36.1</td>
      <td>84.4</td>
      <td>94.0</td>
    </tr>
    <tr>
      <th>White</th>
      <td>214.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>47.21</td>
      <td>31.32</td>
      <td>4.5</td>
      <td>22.3</td>
      <td>42.65</td>
      <td>85.9</td>
      <td>96.4</td>
    </tr>
    <tr>
      <th>Black</th>
      <td>214.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>35.19</td>
      <td>32.36</td>
      <td>1.1</td>
      <td>10.12</td>
      <td>23.2</td>
      <td>70.62</td>
      <td>93.5</td>
    </tr>
    <tr>
      <th>Hispanic</th>
      <td>204.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>27.68</td>
      <td>26.85</td>
      <td>0.6</td>
      <td>7.6</td>
      <td>16.6</td>
      <td>57.02</td>
      <td>87.2</td>
    </tr>
    <tr>
      <th>Asian</th>
      <td>168.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>62.02</td>
      <td>27.03</td>
      <td>15.0</td>
      <td>46.55</td>
      <td>66.0</td>
      <td>81.05</td>
      <td>98.5</td>
    </tr>
    <tr>
      <th>Pacific Islander</th>
      <td>93.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>50.79</td>
      <td>33.24</td>
      <td>10.0</td>
      <td>24.0</td>
      <td>34.0</td>
      <td>90.7</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>American Indian/Alaska Native</th>
      <td>131.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>42.67</td>
      <td>32.1</td>
      <td>2.1</td>
      <td>16.55</td>
      <td>24.4</td>
      <td>83.05</td>
      <td>95.1</td>
    </tr>
    <tr>
      <th>Two or more races</th>
      <td>157.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>44.46</td>
      <td>31.58</td>
      <td>2.9</td>
      <td>25.7</td>
      <td>34.7</td>
      <td>87.9</td>
      <td>98.2</td>
    </tr>
  </tbody>
</table>
</div>
<h1 id="visulization">Visulization</h1>
<h2 id="correlation-matrix-heatmap">Correlation Matrix Heatmap</h2>
<pre><code class="language-Python">cormat = df[race].corr().round(2)
sns.heatmap(cormat, annot=True)
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://qingqiuzhang.github.io/post-images/1648954052583.png" alt="" loading="lazy"></figure>
<h2 id="enrollment-date-distribution">Enrollment Date Distribution</h2>
<pre><code class="language-Python">f, axes = plt.subplots(4, 2, sharey=False, figsize=(15, 12))
for ind, val in enumerate(race):
    sns.histplot(
        data=df,
        x=val,
        hue=&quot;Min degree&quot;,
        label=&quot;100% Equities&quot;,
        kde=True,
        stat=&quot;density&quot;,
        linewidth=0,
        ax=axes[ind // 2, ind % 2],
        bins=200,
    ).set(title=val)
f.tight_layout(pad=3.0)
plt.show()
</code></pre>
<p><img src="https://qingqiuzhang.github.io/post-images/1648954297942.png" alt="" loading="lazy"><br>
For Black, Hispanic, Pacific Islander people and people with two or more races, the enrollment rate is low for most of the past time. For Asian and White people, the overall enrollment rate is high for most of the past time.</p>
<pre><code class="language-Python">f, axes = plt.subplots(4, 2, sharey=False, figsize=(14, 14))
for ind, val in enumerate(race):
    sns.lineplot(
        data=df, x=&quot;Year&quot;, y=val, hue=&quot;Min degree&quot;, ci=None, ax=axes[ind // 2, ind % 2]
    )
</code></pre>
<p><img src="https://qingqiuzhang.github.io/post-images/1648954622712.png" alt="" loading="lazy"><br>
The Black and White people has low enrollment rate at the 1920s to 1970s, but the enrollments increased dramatically. For Black, White and Hispanic people, the enrollment rates for the four degrees has been increasing overtime. The enrollment rates of the four degrees keeps quite static since 2006 for Asian, Pacific Islander, American Indian/Alaska Native and people with two or more races.</p>
<h1 id="questions">Questions</h1>
<h2 id="q1">Q1</h2>
<p>What are the percent of different degrees completed for a given year range and sex? Parameter arguments are as follows: two year arguments, and a value for sex (â€™Aâ€™, â€™Fâ€™, or â€™Mâ€™). Function should return all rows of the data which match the given sex, and have data between the given years (inclusive for the start, exclusive for the end). If no data is found for the parameters, return Python keyword None.</p>
<pre><code class="language-Python">def completion_bet_years(dataframe, year1, year2, sex):
    &quot;&quot;&quot; Return percent of different degrees completed between year1 and year2 for Sex==sex.

    Args:
    dataframe (DataFrame): A dataframe containing the needed data
    year1     (int)      : Year number: the  earlier one
    year2     (int)      : Year number: the  later one
    sex       (str)      : Gender

    Returns:
    DataFrame: The percent of degrees completed between year1 and year2 for Sex==sex.
    &quot;&quot;&quot;
    result = dataframe.loc[
        (dataframe[&quot;Sex&quot;] == sex)
        &amp; (dataframe[&quot;Year&quot;] &gt;= year1)
        &amp; (dataframe[&quot;Year&quot;] &lt; year2)
    ]
    if result.shape[0] == 0:
        return None
    else:
        return result

print(completion_bet_years(df, 1920, 1941, &quot;A&quot;))
</code></pre>
<pre>    Year Sex   Min degree  Total  White  Black  Hispanic  Asian  \
0   1920   A  high school    NaN   22.0    6.3       NaN    NaN   
1   1940   A  high school   38.1   41.2   12.3       NaN    NaN   
39  1920   A   bachelor's    NaN    4.5    1.2       NaN    NaN   
40  1940   A   bachelor's    5.9    6.4    1.6       NaN    NaN   

    Pacific Islander  American Indian/Alaska Native  Two or more races  
0                NaN                            NaN                NaN  
1                NaN                            NaN                NaN  
39               NaN                            NaN                NaN  
40               NaN                            NaN                NaN  
</pre>
<h2 id="q2">Q2</h2>
<p>What were the percentages for women vs men having earned a Bachelorâ€™s Degree in a given year? Parameter list argument is the year in question and return the percentages as a tuple: (% for men, % for women)</p>
<pre><code class="language-Python">def compare_bachelors_in_year(dataframe, year):
    &quot;&quot;&quot; Return the percentages for women vs men having earned a Bachelorâ€™s Degree in Year==year

    Args:
    df   (Dataframe): A dataframe containing the needed data
    year (int)      : The year in which you want to know the info

    Returns:
    tuple: A tuple returns the percentage of Bachelorâ€™s Degree male and femal in Year==year.
    &quot;&quot;&quot;
    women = dataframe[
        (dataframe[&quot;Sex&quot;] == &quot;F&quot;)
        &amp; (dataframe[&quot;Min degree&quot;] == &quot;bachelor's&quot;)
        &amp; (dataframe[&quot;Year&quot;] == year)
    ].iloc[0][&quot;Total&quot;]
    men = dataframe[
        (dataframe[&quot;Sex&quot;] == &quot;M&quot;)
        &amp; (dataframe[&quot;Min degree&quot;] == &quot;bachelor's&quot;)
        &amp; (dataframe[&quot;Year&quot;] == year)
    ].iloc[0][&quot;Total&quot;]
    return (f&quot;{men} % for men&quot;, f&quot;{women} % for women&quot;)

compare_bachelors_in_year(df, 2010)
</code></pre>
<pre>('27.8 % for men', '35.7 % for women')</pre>
<h2 id="q3">Q3</h2>
<p>What were the two most commonly awarded levels of educational attainment awarded between 2000-2010 (inclusive)? Use the mean percent over the years to compare the education levels. Return a list as follows: [#1 level, mean % of #1 level, #2 level, mean % of #2 level].</p>
<pre><code class="language-Python">def top_2_2000s(dataframe):
    &quot;&quot;&quot;Return the two most common educational attainment between 2000-2010.

    Args:
    DataFrame (DataFrame): A dataframe containing the needed data

    Returns:
    list: A list returns the two most common educational attainment between 2000-2010.
    &quot;&quot;&quot;
    df_2000s = dataframe.loc[
        (dataframe[&quot;Year&quot;] &gt;= 2000)
        &amp; (dataframe[&quot;Year&quot;] &lt;= 2010)
        &amp; (dataframe[&quot;Sex&quot;] == &quot;A&quot;)
    ][[&quot;Total&quot;, &quot;Min degree&quot;]]
    mean_percent_edu = (
        df_2000s.groupby(&quot;Min degree&quot;).mean().sort_values(by=&quot;Total&quot;, ascending=False)
    )
    level1 = mean_percent_edu.iloc[0][&quot;Total&quot;].round(2)
    level2 = mean_percent_edu.iloc[1][&quot;Total&quot;].round(2)
    edu1 = mean_percent_edu.index[0]
    edu2 = mean_percent_edu.index[1]
    return [f&quot;#1 level, {level1} % of {edu1}&quot;, f&quot;#2 level, {level2} % of {edu2}&quot;]

top_2_2000s(df)

</code></pre>
<pre>['#1 level, 87.557 % of high school', "#2 level, 38.757 % of associate's"]</pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diabetes Data Analytics (1) -- Data Preprocessing]]></title>
        <id>https://qingqiuzhang.github.io/diabetes-dataset-report/</id>
        <link href="https://qingqiuzhang.github.io/diabetes-dataset-report/">
        </link>
        <updated>2022-02-17T19:44:03.000Z</updated>
        <summary type="html"><![CDATA[<p><img src="https://qingqiuzhang.github.io/post-images/1649601809050.jpg" alt="" loading="lazy"><br>
This is a thorough analysis of diabetes dataset from Kaggle. In this article, I will demonstrate how to do an end-to-end machine learning project using diverse classification models.<br>
<strong>In case you need the source data, visit this webpage:</strong> <a href="https://www.kaggle.com/datasets/mathchi/diabetes-data-set">Data Source</a></p>
]]></summary>
        <content type="html"><![CDATA[<p><img src="https://qingqiuzhang.github.io/post-images/1649601809050.jpg" alt="" loading="lazy"><br>
This is a thorough analysis of diabetes dataset from Kaggle. In this article, I will demonstrate how to do an end-to-end machine learning project using diverse classification models.<br>
<strong>In case you need the source data, visit this webpage:</strong> <a href="https://www.kaggle.com/datasets/mathchi/diabetes-data-set">Data Source</a></p>
<!-- more -->
<h1 id="what-can-we-do-with-this-dataset">What Can We Do With This Dataset?</h1>
<p>The observations in the dataset are all females from Pima Indian heritage who are greater than or equal to 21 years old.</p>
<h1 id="data-preprocessing">Data Preprocessing</h1>
<p>I cannot emphasize the importance of data preprocessing. It is the process of transforming raw data into more reasonable, useful and efficient format. It's a must and one of the most important step in a machine learing project. Without it our models will probably crash and won't be able to generate good results.<br>
There are several steps in data preprocessing: data cleaning, data reduction and data transformation. In the following paragraphs, I'll show you how to perform the steps one by one.</p>
<h2 id="data-cleaning">Data Cleaning</h2>
<pre><code class="language-Python3"># packages needed
import numpy as np
import pandas as pd

# load data
data = pd.read_csv('diabetes.csv')
data.head(10)
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>116</td>
      <td>74</td>
      <td>0</td>
      <td>0</td>
      <td>25.6</td>
      <td>0.201</td>
      <td>30</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3</td>
      <td>78</td>
      <td>50</td>
      <td>32</td>
      <td>88</td>
      <td>31.0</td>
      <td>0.248</td>
      <td>26</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>115</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35.3</td>
      <td>0.134</td>
      <td>29</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2</td>
      <td>197</td>
      <td>70</td>
      <td>45</td>
      <td>543</td>
      <td>30.5</td>
      <td>0.158</td>
      <td>53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>8</td>
      <td>125</td>
      <td>96</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.232</td>
      <td>54</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
<p>The meaning of each column is stated as below:</p>
<blockquote>
<ul>
<li>Pregnancies: Number of times pregnant</li>
<li>Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li>
<li>BloodPressure: Diastolic blood pressure (mm Hg)</li>
<li>SkinThickness: Triceps skin fold thickness (mm)</li>
<li>Insulin: 2-Hour serum insulin (mu U/ml)</li>
<li>BMI: Body mass index (weight in kg/(height in m)^2)</li>
<li>DiabetesPedigreeFunction: Diabetes pedigree function</li>
<li>Age: Age (years)</li>
<li>Outcome: Class variable (0 or 1)</li>
</ul>
</blockquote>
<p>Next, we need to examin whether there are null values in the dataset.</p>
<pre><code class="language-Python">data.isnull().sum()
</code></pre>
<pre>Pregnancies                 0
Glucose                     0
BloodPressure               0
SkinThickness               0
Insulin                     0
BMI                         0
DiabetesPedigreeFunction    0
Age                         0
Outcome                     0
dtype: int64</pre>
<p>It seems there is no null values. Shall we pop the champagne now? If we further examine the data, we will find that the situation is not as good as we thought.</p>
<pre><code class="language-Python">data.describe().round(2)
</code></pre>
<div style="overflow: auto; width: 780px">
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Pregnancies</th>
      <td>768.0</td>
      <td>3.85</td>
      <td>3.37</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>3.00</td>
      <td>6.00</td>
      <td>17.00</td>
    </tr>
    <tr>
      <th>Glucose</th>
      <td>768.0</td>
      <td>120.89</td>
      <td>31.97</td>
      <td>0.00</td>
      <td>99.00</td>
      <td>117.00</td>
      <td>140.25</td>
      <td>199.00</td>
    </tr>
    <tr>
      <th>BloodPressure</th>
      <td>768.0</td>
      <td>69.11</td>
      <td>19.36</td>
      <td>0.00</td>
      <td>62.00</td>
      <td>72.00</td>
      <td>80.00</td>
      <td>122.00</td>
    </tr>
    <tr>
      <th>SkinThickness</th>
      <td>768.0</td>
      <td>20.54</td>
      <td>15.95</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>23.00</td>
      <td>32.00</td>
      <td>99.00</td>
    </tr>
    <tr>
      <th>Insulin</th>
      <td>768.0</td>
      <td>79.80</td>
      <td>115.24</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>30.50</td>
      <td>127.25</td>
      <td>846.00</td>
    </tr>
    <tr>
      <th>BMI</th>
      <td>768.0</td>
      <td>31.99</td>
      <td>7.88</td>
      <td>0.00</td>
      <td>27.30</td>
      <td>32.00</td>
      <td>36.60</td>
      <td>67.10</td>
    </tr>
    <tr>
      <th>DiabetesPedigreeFunction</th>
      <td>768.0</td>
      <td>0.47</td>
      <td>0.33</td>
      <td>0.08</td>
      <td>0.24</td>
      <td>0.37</td>
      <td>0.63</td>
      <td>2.42</td>
    </tr>
    <tr>
      <th>Age</th>
      <td>768.0</td>
      <td>33.24</td>
      <td>11.76</td>
      <td>21.00</td>
      <td>24.00</td>
      <td>29.00</td>
      <td>41.00</td>
      <td>81.00</td>
    </tr>
    <tr>
      <th>Outcome</th>
      <td>768.0</td>
      <td>0.35</td>
      <td>0.48</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
    </tr>
  </tbody>
</table>
</div>
We can see from the picture above that the minimum value of Glucose, BloodPressure, SkinThickness, Insulin, BMI is zero. However, anyone having common sense knows that by no means would these indicators of an alive person be zero. Apparently, the null values are replaced by zeros when inputing data. Here we calculate the number of null values in each column so that we will better decide how to deal with them.
```Python3
for col in list(data.columns):
    zero = len(data[data[col] == 0])
    print('0s in %s: %d' % (col, zero))
```
<pre>0s in Pregnancies: 111
0s in Glucose: 5
0s in BloodPressure: 35
0s in SkinThickness: 227
0s in Insulin: 374
0s in BMI: 11
0s in DiabetesPedigreeFunction: 0
0s in Age: 0
0s in Outcome: 500
</pre>
There are nearly 400 null values in Insulin column, so it's not realistic to drop all the null values. Normally under such circumstance, we replace them with indicators such as mean, median, or mode, etc. Here I choose median.
```Python3
# replace 0s in target columns with null value
data.iloc[:, 1:6] = data.iloc[:, 1:6].replace({0:np.NaN})
# fill null values with medians
data = data.fillna(data.median()) 
data.describe().round(2).T
```
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Pregnancies</th>
      <td>768.0</td>
      <td>3.85</td>
      <td>3.37</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>3.00</td>
      <td>6.00</td>
      <td>17.00</td>
    </tr>
    <tr>
      <th>Glucose</th>
      <td>768.0</td>
      <td>121.66</td>
      <td>30.44</td>
      <td>44.00</td>
      <td>99.75</td>
      <td>117.00</td>
      <td>140.25</td>
      <td>199.00</td>
    </tr>
    <tr>
      <th>BloodPressure</th>
      <td>768.0</td>
      <td>72.39</td>
      <td>12.10</td>
      <td>24.00</td>
      <td>64.00</td>
      <td>72.00</td>
      <td>80.00</td>
      <td>122.00</td>
    </tr>
    <tr>
      <th>SkinThickness</th>
      <td>768.0</td>
      <td>29.11</td>
      <td>8.79</td>
      <td>7.00</td>
      <td>25.00</td>
      <td>29.00</td>
      <td>32.00</td>
      <td>99.00</td>
    </tr>
    <tr>
      <th>Insulin</th>
      <td>768.0</td>
      <td>140.67</td>
      <td>86.38</td>
      <td>14.00</td>
      <td>121.50</td>
      <td>125.00</td>
      <td>127.25</td>
      <td>846.00</td>
    </tr>
    <tr>
      <th>BMI</th>
      <td>768.0</td>
      <td>32.46</td>
      <td>6.88</td>
      <td>18.20</td>
      <td>27.50</td>
      <td>32.30</td>
      <td>36.60</td>
      <td>67.10</td>
    </tr>
    <tr>
      <th>DiabetesPedigreeFunction</th>
      <td>768.0</td>
      <td>0.47</td>
      <td>0.33</td>
      <td>0.08</td>
      <td>0.24</td>
      <td>0.37</td>
      <td>0.63</td>
      <td>2.42</td>
    </tr>
    <tr>
      <th>Age</th>
      <td>768.0</td>
      <td>33.24</td>
      <td>11.76</td>
      <td>21.00</td>
      <td>24.00</td>
      <td>29.00</td>
      <td>41.00</td>
      <td>81.00</td>
    </tr>
    <tr>
      <th>Outcome</th>
      <td>768.0</td>
      <td>0.35</td>
      <td>0.48</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>1.00</td>
    </tr>
  </tbody>
</table>
Now we can visualize the dataset and see if we can find something else.
<pre><code class="language-Python3">import seaborn as sns
sns.pairplot(data, hue=&quot;Outcome&quot;, corner=True)
</code></pre>
<p><img src="https://qingqiuzhang.github.io/post-images/1648690175972.png" alt="Pairplot-Imbalanced data set" loading="lazy"><br>
From the pairplot above, we discover this is an imbalanced dataset. There are 500 observations whose outcome is 0 and only 268 is 1. If we train our model with such data, the results may be misleading. There are several methods to deal with imbalanced data:</p>
<ol>
<li>Collect more data</li>
<li>Under-Sampling</li>
<li>Over-Sampling</li>
<li>Use confusion matrix or other way to evaluate the peformance<br>
As the number of observations is only 768, it's unrealistic to under-sampling. Here I wil illustrate how to do oversampling using SMOTE().</li>
</ol>
<pre><code class="language-Python3">from imblearn.over_sampling import SMOTE 
# get X, y
X = data.iloc[:, 0:8]
y = data.iloc[:, -1]
# perform SMOTE()
oversample = SMOTE()
X, y = oversample.fit_resample(X, y)
# join X, y
y = pd.DataFrame(y)
data = pd.concat([X, y], axis=1)
print(data.shape)
data.describe().round(2).T
</code></pre>
<table border="1">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Pregnancies</th>
      <td>1000.0</td>
      <td>3.99</td>
      <td>3.33</td>
      <td>0.00</td>
      <td>1.00</td>
      <td>3.00</td>
      <td>6.00</td>
      <td>17.00</td>
    </tr>
    <tr>
      <th>Glucose</th>
      <td>1000.0</td>
      <td>126.40</td>
      <td>31.39</td>
      <td>44.00</td>
      <td>102.82</td>
      <td>122.00</td>
      <td>148.00</td>
      <td>199.00</td>
    </tr>
    <tr>
      <th>BloodPressure</th>
      <td>1000.0</td>
      <td>72.97</td>
      <td>11.71</td>
      <td>24.00</td>
      <td>65.75</td>
      <td>72.00</td>
      <td>80.00</td>
      <td>122.00</td>
    </tr>
    <tr>
      <th>SkinThickness</th>
      <td>1000.0</td>
      <td>29.64</td>
      <td>8.22</td>
      <td>7.00</td>
      <td>27.00</td>
      <td>29.00</td>
      <td>33.00</td>
      <td>99.00</td>
    </tr>
    <tr>
      <th>Insulin</th>
      <td>1000.0</td>
      <td>146.58</td>
      <td>90.69</td>
      <td>14.00</td>
      <td>125.00</td>
      <td>125.00</td>
      <td>136.25</td>
      <td>846.00</td>
    </tr>
    <tr>
      <th>BMI</th>
      <td>1000.0</td>
      <td>33.01</td>
      <td>6.68</td>
      <td>18.20</td>
      <td>28.40</td>
      <td>32.66</td>
      <td>36.80</td>
      <td>67.10</td>
    </tr>
    <tr>
      <th>DiabetesPedigreeFunction</th>
      <td>1000.0</td>
      <td>0.49</td>
      <td>0.32</td>
      <td>0.08</td>
      <td>0.26</td>
      <td>0.40</td>
      <td>0.65</td>
      <td>2.42</td>
    </tr>
    <tr>
      <th>Age</th>
      <td>1000.0</td>
      <td>33.88</td>
      <td>11.34</td>
      <td>21.00</td>
      <td>25.00</td>
      <td>31.00</td>
      <td>41.00</td>
      <td>81.00</td>
    </tr>
    <tr>
      <th>Outcome</th>
      <td>1000.0</td>
      <td>0.50</td>
      <td>0.50</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.50</td>
      <td>1.00</td>
      <td>1.00</td>
    </tr>
  </tbody>
</table>
Once again visualize the dataset to see the changes in the distribution of data.
<pre><code class="language-Python3">sns.pairplot(data, hue=&quot;Outcome&quot;, corner=True)
</code></pre>
<p><img src="https://qingqiuzhang.github.io/post-images/1648950949751.png" alt="pairplot2" loading="lazy"><br>
For now, we transform the original imbalanced dataset into a balanced dataset with the number of each outcome class being 500. there is not much difference in distribution between diabetics and nondiabetics. Glucose should have the highest correlation with Outcome among all the features. We can testify it using a correlation matrix heatmap.</p>
<pre><code class="language-Python3">corr_mat = data.corr().round(2)
plt.figure(figsize = (15,10))
sns.heatmap(corr_mat, annot=True)
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://qingqiuzhang.github.io/post-images/1648951579451.png" alt="" loading="lazy"></figure>
<h2 id="data-transformation">Data Transformation</h2>
<p>If the range of two features differ a lot, for example, one is from 0 to 1 and the other is from 0 to 10,000, apparantly models won't treat such two features equally. Normally, the feature with larger range would influence the result more. Nevertheless, this doesn't mean it's more important than the other one. That's why we need scale the features. Frequently used scaling techniques are:</p>
<ul>
<li>MinMaxScaler()</li>
<li>StardardScaler()</li>
<li>MaxAbsScaler()</li>
<li>RobustScaler()</li>
</ul>
<p>I usually choose MinMaxScaler() or StandardScaler() for most of datasets. You can also use others according to the charasteristic of the dataset at hand.</p>
<pre><code class="language-Python3"># get X, y
X = data.drop([&quot;Outcome&quot;], axis=1)
y = data[&quot;Outcome&quot;]

X2 = X.copy
y2 = y.copy

from sklearn.model_selection import train_test_split
# split X, y into traning set and validation set
X2_train, X2_trainValid, y2_train, y2_trainValid = train_test_split(X, y, test_size=0.5, stratify=y)
# perform feature scaling
scaler = StandardScaler()
X2_train = scaler.fit_transform(X2_train)
print(X2_train)
X2_trainValid = scaler.transform(X2_trainValid)
print(X2_trainValid)
</code></pre>
<pre>[[-0.86583338 -1.18590252 -2.5407598  ...  2.36236055  0.059368
  -0.66182567]
 [-0.56074762 -1.30437961 -0.5856297  ...  0.92385297  1.5766631
  -0.575063  ]
 [-0.56074762 -0.84741167 -1.01001675 ...  0.49700038 -0.09947556
  -1.09563904]
 ...
 [ 0.35450965  0.45821104  0.68753147 ... -0.21442061 -0.42275597
  -0.74858835]
 [ 1.57485267  1.18460674 -0.30940607 ... -1.09502236 -0.52771647
   1.07342779]
 [-0.56074762 -0.8147711  -0.33099746 ...  0.76734036 -0.55277092
  -0.66182567]]
[[ 1.57485267 -0.73832382  1.59549311 ...  1.66775686  1.24296528
   0.55285175]
 [-0.86583338 -1.43494188  0.17826701 ...  1.8771571   2.15997341
  -0.14124963]
 [-0.86583338  2.09023942 -1.01001675 ... -0.4278469  -0.29274103
   2.20134255]
 ...
 [-0.86583338 -1.50022302  0.68753147 ... -1.09658263  0.35733372
  -1.00887637]
 [-1.17091913 -0.03139747 -0.16124264 ... -0.81201424 -0.79874515
   0.20580106]
 [-1.17091913 -1.66342586 -1.68903605 ... -0.75510056 -0.74603639
  -1.00887637]]
</pre>
<h2 id="data-reduction">Data Reduction</h2>
<p>With clean data at hand, we can visulize the data, which may indicate some relations between the features or between features and targets. Knowing these is useful to data reduction, for example, if a feature is highly correlated with the target, then we had better retain it. Or if two features are highly correlated, it would be much easier for us if we drop out one of them. Here I use PCA (Principal componenet analysis) for a demonstration.</p>
<pre><code class="language-Python3">from sklearn.decomposition import PCA
pca = PCA()
X = pca.fit_transform(X)
print(X)
</code></pre>
<pre>[[-1.48212827e+01  2.66418913e+01  6.77045733e+00 ...  1.28306645e+00
  -4.41730071e-01  1.08702215e-01]
 [-2.53614440e+01 -3.80054541e+01 -1.53466357e+00 ...  4.06783776e+00
  -2.50459441e+00 -9.57021831e-02]
 [-9.83659118e+00  5.72071616e+01 -1.78166537e+01 ...  8.30157785e+00
   4.60163646e+00  2.42241723e-01]
 ...
 [-8.54656132e+01  3.00921079e+01  6.86126007e+00 ...  4.57134748e+00
   2.42255141e+00  1.22062151e-01]
 [-2.36693301e+01 -2.79343649e+01 -3.54949033e+00 ... -3.82012635e-01
   4.69302230e-01 -2.89569829e-01]
 [-1.69091697e+01  1.34152393e+01  3.35830801e+00 ... -1.66109447e+01
   5.12758994e+00  2.13862262e-02]]
</pre>
]]></content>
    </entry>
</feed>